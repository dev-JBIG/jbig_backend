<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>로지스틱 회귀</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.collection-content td {
	white-space: pre-wrap;
	word-break: break-word;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

blockquote.quote-large {
	font-size: 1.25em;
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.highlight-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.highlight-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.highlight-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.highlight-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.highlight-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.highlight-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.highlight-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.highlight-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.highlight-default_background {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray_background {
	background: rgba(240, 239, 237, 1);
}
.highlight-brown_background {
	background: rgba(245, 237, 233, 1);
}
.highlight-orange_background {
	background: rgba(251, 235, 222, 1);
}
.highlight-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.highlight-teal_background {
	background: rgba(232, 241, 236, 1);
}
.highlight-blue_background {
	background: rgba(232, 242, 250, 1);
}
.highlight-purple_background {
	background: rgba(243, 235, 249, 1);
}
.highlight-pink_background {
	background: rgba(250, 233, 241, 1);
}
.highlight-red_background {
	background: rgba(252, 233, 231, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.block-color-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.block-color-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.block-color-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.block-color-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.block-color-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.block-color-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.block-color-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.block-color-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(240, 239, 237, 1);
}
.block-color-brown_background {
	background: rgba(245, 237, 233, 1);
}
.block-color-orange_background {
	background: rgba(251, 235, 222, 1);
}
.block-color-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.block-color-teal_background {
	background: rgba(232, 241, 236, 1);
}
.block-color-blue_background {
	background: rgba(232, 242, 250, 1);
}
.block-color-purple_background {
	background: rgba(243, 235, 249, 1);
}
.block-color-pink_background {
	background: rgba(250, 233, 241, 1);
}
.block-color-red_background {
	background: rgba(252, 233, 231, 1);
}
.select-value-color-default { background-color: rgba(42, 28, 0, 0.07); }
.select-value-color-gray { background-color: rgba(28, 19, 1, 0.11); }
.select-value-color-brown { background-color: rgba(127, 51, 0, 0.156); }
.select-value-color-orange { background-color: rgba(196, 88, 0, 0.203); }
.select-value-color-yellow { background-color: rgba(209, 156, 0, 0.282); }
.select-value-color-green { background-color: rgba(0, 96, 38, 0.156); }
.select-value-color-blue { background-color: rgba(0, 99, 174, 0.172); }
.select-value-color-purple { background-color: rgba(92, 0, 163, 0.141); }
.select-value-color-pink { background-color: rgba(183, 0, 78, 0.152); }
.select-value-color-red { background-color: rgba(206, 24, 0, 0.164); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1b44d778-1cdc-80d6-89c4-ca3511d3065d" class="page sans"><header><h1 class="page-title">로지스틱 회귀</h1><p class="page-description"></p></header><div class="page-body"><h1 id="1b44d778-1cdc-80c2-8665-d2731f0581bd" class="">개념</h1><h1 id="1b44d778-1cdc-8071-ab52-faf224e6f788" class="">로지스틱 회귀란?</h1><p id="1b44d778-1cdc-80af-b184-c1bf14bd650a" class="">로지스틱 회귀 알고리즘은 2진 분류 모델로 사용되고 있다.</p><p id="1b44d778-1cdc-8023-840f-faed6af67862" class="">따라서 <strong>로지스틱 회귀(Logistic Regression)</strong>는 회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0과 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘이다.</p><p id="1b44d778-1cdc-806b-9d73-ed493d7db7bb" class="">스팸 메일 분류기 같은 예시를 생각하면 쉽다. 어떤 메일을 받았을 때 그것이 스팸일 확률이 0.5 이상이면 spam으로 분류하고, 확률이 0.5보다 작은 경우 ham으로 분류하는거다. 이렇게 데이터가 2개의 범주 중 하나에 속하도록 결정하는 것을 2<strong>진 분류(binary classification)</strong>라고 한다.</p><p id="1b44d778-1cdc-80bf-b53d-d901788af198" class="">로지스틱 회귀를 이해하려면 우선 선형 회귀(Linear Regression)에 대한 개념을 익혀야 한다.</p><p id="1b44d778-1cdc-809e-9d6b-c18fd6118cd2" class="">예를 들어 어떤 학생이 공부하는 시간에 따라 시험에 합격할 확률이 달라진다고 해보자. <strong>선형 회귀를 사용하면</strong> 아래와 같은 그림으로 나타낼 수 있다.</p><figure id="1b44d778-1cdc-8083-9078-cd45dc9834a7" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F2b87ad5d-bd8e-43b8-887e-dd8a76bb2ccb%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F2b87ad5d-bd8e-43b8-887e-dd8a76bb2ccb%2Fimage.png"/></a></figure><p id="1b44d778-1cdc-80f2-a24e-e82feb25c856" class="">공부한 시간이 적으면 시험에 통과 못하고, 공부한 시간이 많으면 시험에 통과한다는 식으로 설명할 수 있다. 그런데 이 회귀선을 자세히 살펴보면 <strong>확률이 음과 양의 방향으로 무한대까지 뻗어 간다.</strong> 말 그대로 &#x27;선&#x27;이라서.</p><p id="1b44d778-1cdc-8035-8050-d330e5f28689" class="">그래서 공부를 2시간도 안하면 시험에 통과할 확률이 0이 안된다. 이건 말이 안된다.</p><p id="1b44d778-1cdc-8095-b8ed-f171419ddfd2" class="">만약 <strong>로지스틱 회귀</strong>를 사용하면 아래와 같이 나타난다.</p><figure id="1b44d778-1cdc-809c-b48d-cd12f3ac5c43" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fe53ce299-c1d4-4c8c-852a-007489d18236%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fe53ce299-c1d4-4c8c-852a-007489d18236%2Fimage.png"/></a></figure><p id="1b44d778-1cdc-8013-b8c7-ee81dfa717ea" class="">시험에 합격할 확률이 0과 1 사이의 값으로 그려진다. 이제야 좀 납득이 간다.</p><p id="1b44d778-1cdc-8042-8989-c460c569342a" class="">
</p><h3 id="1b44d778-1cdc-8077-8f0e-f0b80b77f585" class=""><strong>Classification Threshold (임계값)</strong></h3><p id="1b44d778-1cdc-80f8-bda5-f01bc195b25e" class="">이렇게 로지스틱 회귀 알고리즘의 결과 값은 &#x27;분류 확률&#x27;이고, 그래서 이 확률이 특정 수준 이상 확보되면 샘플이 그 클래스에 속할지 말지 결정할 수 있다.</p><p id="1b44d778-1cdc-80fe-ba2a-c97f568d72fa" class="">그리고 당연히 대부분 알고리즘에서 기본 임계값은 <strong>0.5</strong>다.</p><p id="1b44d778-1cdc-8022-8f7c-fb15b47ca7bb" class="">다만, 필요에 따라 모델의 임계값을 변경할 수 있다. 예를 들어, 암을 진단하는 로지스틱 회귀 모델을 작성하는 경우에는 혹시 모를 경우에 대비하여 좀 더 민감하게 확인하기 위해 0.3이나 0.4로 임계값을 낮춰 모델의 민감도를 높일 필요가 있다. 그래야 전체적으로 오분류가 더 많아지더라도 실제 암 환자를 놓치는 사례는 적어질 테니까.</p><figure id="1b44d778-1cdc-8027-ba2a-c06a1859381f" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fa5475bf9-674a-4c9d-b2c6-e58479558e65%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fa5475bf9-674a-4c9d-b2c6-e58479558e65%2Fimage.png"/></a></figure><figure id="1b44d778-1cdc-80ea-8bb9-d1d207722b1a" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F4d304ab3-8a1d-4ef2-87a2-96065d6cf00c%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F4d304ab3-8a1d-4ef2-87a2-96065d6cf00c%2Fimage.png"/></a></figure><figure id="1b94d778-1cdc-8050-ba31-fb3aa93a0191"><div class="source"><a href="https://youtu.be/m7oSyX4QCwY?si=WyhHvljtOxMgSKOp">https://youtu.be/m7oSyX4QCwY?si=WyhHvljtOxMgSKOp</a></div></figure><h2 id="1b44d778-1cdc-809f-87b7-ff090a4a6d1d" class="">요약</h2><ul id="1b44d778-1cdc-807c-8587-cb1315ec51a0" class="bulleted-list"><li style="list-style-type:disc">로지스틱 회귀 분석은 이진 분류를 수행하는 데 사용된다. 즉, 데이터 샘플을 양성(1) 또는 음성(0) 클래스 둘 중 어디에 속하는지 예측한다.</li></ul><ul id="1b44d778-1cdc-8036-a228-cde3d23bfc09" class="bulleted-list"><li style="list-style-type:disc">각 속성(feature)들의 계수 log-odds를 구한 후 sigmoid 함수를 적용하여 실제로 데이터가 해당 클래스에 속할 확률을 0과 1사이의 값으로 나타낸다.</li></ul><ul id="1b44d778-1cdc-80fd-959d-f6cc0b411e8a" class="bulleted-list"><li style="list-style-type:disc">데이터가 클래스에 속할지 말지 결정할 확률 컷오프를 Threshold(임계값)이라 한다. 기본 값은 0.5지만 데이터의 특성이나 상황에 따라 조정할 수 있다.</li></ul><h1 id="2634d778-1cdc-8010-9224-c736682dc454" class="">로지스틱 회귀도 다중분류가 가능하다?</h1><p id="1b44d778-1cdc-801c-b868-c798241210f6" class="">로지스틱회귀는 사실 클래스를 3개 이상으로 분류하는 다중분류(Multinomial Calssification)도 처리할 수 있습니다. </p><p id="1b44d778-1cdc-806e-9189-e744628304a4" class="">이진분류는 하나의 선형방정식과 하나의 확률값이 나와 이를 기준으로 데이터를 분류하는 것입니다.</p><p id="1b44d778-1cdc-80b6-8619-ce3efa992354" class="">그에 반해 다중 분류는 각 클래스별로 선형방정식이 나오고 그에 따른 확률값이 계산됩니다. </p><p id="1b44d778-1cdc-8064-9ac9-f86f1542a6af" class="">바꿔 말하면 3개 클래스로 예측하는 경우 각 데이터 샘플에 대해 3개의 선형방정식과 그에 따른 3개의 확률값이 나오고 그 중에 가장 큰 확률값을 가진 클래스로 예측되는 것입니다.</p><figure id="1b44d778-1cdc-8066-a408-e6e655c91cf9" class="image"><a href="image.png"><img style="width:337.98809814453125px" src="image.png"/></a></figure><ul id="1ba4d778-1cdc-80b1-9267-dc2e441fe36a" class="toggle"><li><details open=""><summary><mark class="highlight-default">이진분류와의 원리적 차이</mark></summary><p id="42e7ea38-f90f-4c30-94e9-2f3375046cd7" class="">그렇다면 하나의 확률값을 만들어 나머지 클래스는 자동으로 (1-확률값)이 되는 시그모이드가 아니라 각각의 선형방정식에서 나온 결과값들을 종합해 확률로 변환하는 함수가 필요합니다.</p><p id="cce86d06-ed0a-4ada-8e88-c822fec64329" class="">바로 소프트맥스(SoftMax) 함수인데 소프트맥스는 선형방정식에서 나온 결과값(z)을 자연상수를 밑으로 하는 지수함수의 지수로 사용하고 그 값들을 모두 더해 분모로 하고 각 결과값의 지수함수값을 분자로 그 비중에 따라 0-1 사이의 확률로 변환해줍니다.</p><figure id="024afedd-8b0b-4ca4-8c20-58c383db4e25" class="image"><a href="image%201.png"><img style="width:567.9921875px" src="image%201.png"/></a></figure><p id="69c6a03e-905a-4246-9b06-c6a0e4d31bf1" class="">z는 실수값의 입력 벡터</p><p id="312299e8-b520-4aa2-99a4-783b2745a3b8" class="">K는 분류 클래스 수</p><p id="ebab57a2-b7df-46b2-9b85-5c54c4d07eed" class="">e는 자연로그(오일러 수)의 밑</p><figure id="54c7fedc-9999-4b85-81ed-e0dea0180c24" class="image"><a href="image%202.png"><img style="width:197.99444580078125px" src="image%202.png"/></a></figure><h3 id="e5e6718b-4636-4d19-aab3-d560fe6b4aa8" class="">시그모이드와 소프트맥스의 차이점</h3><figure id="38843259-693a-45f9-93f5-742bcc705505" class="image"><a href="image%203.png"><img style="width:567.9921875px" src="image%203.png"/></a></figure><p id="b3501bb3-6bcd-42dd-827f-bdf95e3db472" class="">언뜻 보기에 시그모이드 함수와 소프트맥스 함수는 두 함수 모두 입력 값을 0과 1 사이의 숫자 범위에 매핑하기 때문에 비교적 비슷해 보입니다. </p><p id="9f12c042-b642-4d53-a44e-5367b4bca486" class="">또한 시그모이드 함수는 x = 0에서 0.5 값을 통과하고 소프트맥스 함수는 이 시점에서 여전히 0.5 미만이라는 차이점을 제외하고는 거의 동일한 과정을 거칩니다.</p><p id="6214f879-6a74-4a87-a5d6-cc3451a31208" class="">두 함수의 차이점은 적용에 있습니다. </p><p id="ca9bcb01-d585-4f90-9147-e5a3cd8f7276" class="">시그모이드 함수는 이진 분류, 즉 서로 다른 두 클래스 사이에서 결정을 내려야 하는 모델에 사용할 수 있습니다. </p><p id="eefe3f0f-c064-42e3-a4b5-34aa98ff34b9" class="">반면 소프트맥스는 두 개 이상의 클래스를 예측해야 하는 분류에도 사용할 수 있습니다. 이 경우 이 함수는 모든 클래스의 확률이 1이 되도록 보장합니다.</p><p id="d8b64262-ce9d-4bcd-8e62-1e381c9610d6" class="">실제로 두 클래스의 경우 시그모이드 함수와 소프트맥스 함수가 일치한다는 것을 수학적으로 증명할 수도 있습니다.</p><p id="e51bc1d5-f465-477f-9fa7-e4aab98a9851" class="">
</p><p id="17ad9803-449c-4f85-9d91-05e945af41ec" class="">시그모이드, 소프트맥스는 딥러닝에서도 중요한 역할을 하니 잘 알아두세요!!</p><h2 id="cdd10351-5fda-48ce-b382-634eac442a99" class="">간단 정리 !</h2><h3 id="96aa9279-c608-446e-ab6f-688396638747" class="">시그모이드 - 이진 분류 모델의 마지막 활성화 함수 !</h3><h3 id="6c2d9d70-d8fc-49e1-9e74-d45b3d22f149" class="">소프트맥스 - 다중 분류 모델의 마지막 활성화 함수 !</h3></details></li></ul><ul id="1b44d778-1cdc-80e3-aab8-dc712989bc81" class="toggle"><li><details open=""><summary><mark class="highlight-default">코드</mark></summary><p id="1b44d778-1cdc-8022-bf0f-dcd45db1da57" class="">이진분류(Sigmoid)</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-809a-ba60-f807636483a2" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
data = load_iris()
X = data[&#x27;data&#x27;][:100]  # Iris setosa와 Iris versicolor만 선택
y = data[&#x27;target&#x27;][:100]

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 로지스틱 회귀 모델 생성 및 훈련
model = LogisticRegression()
model.fit(X_train, y_train)

# 예측 및 성능 평가
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f&#x27;Accuracy: {accuracy:.2f}&#x27;)
</code></pre><p id="1b44d778-1cdc-80b9-916c-f1fbc23d8f2e" class="">다중분류(SoftMax)</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-80f5-abe8-d35fc3ea49f9" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
data = load_iris()
X = data[&#x27;data&#x27;]
y = data[&#x27;target&#x27;]

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Softmax 회귀 모델 생성 및 훈련
model = LogisticRegression(multi_class=&#x27;multinomial&#x27;,max_iter=200)
model.fit(X_train, y_train)

# 예측 및 성능 평가
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f&#x27;Accuracy: {accuracy:.2f}&#x27;)
</code></pre><p id="1b44d778-1cdc-80f5-b399-d2ea81f9f38b" class="">
</p><p id="1b44d778-1cdc-80c6-b65f-d17e743b54b0" class="">근데 사실 공식홈페이지를 확인해보니 auto가 default값이라 따로 설정안해줘도 되겠네요ㅎㅎ</p><figure id="1b44d778-1cdc-80be-b678-c34e49ccf914" class="image"><a href="image%204.png"><img style="width:427px" src="image%204.png"/></a></figure><p id="1b44d778-1cdc-80ee-a8be-c471a14fed3d" class="">
</p><ul id="1b44d778-1cdc-80bf-81b1-c5874f1e4998" class="toggle"><li><details open=""><summary>하이퍼 파라미터s (추가 자료)</summary><ul id="1b44d778-1cdc-8079-bd62-ccae392762c5" class="bulleted-list"><li style="list-style-type:disc"><strong><span style="border-bottom:0.05em solid">하나하나 다 공부할 필요까지는 없고 읽어보면서 이런것들을 조정해서 모델 성능을 올릴수 있구나까지 생각해도 충분합니다.</span></strong></li></ul><ul id="1b44d778-1cdc-809d-97d4-ed3426f225f6" class="bulleted-list"><li style="list-style-type:disc"><strong>max_iterint, default=100</strong><p id="1b44d778-1cdc-8052-bed3-e40c8ad396e2" class="">Maximum number of iterations taken for the solvers to converge.</p></li></ul><ul id="1b44d778-1cdc-80f8-9072-c9c44b2ac785" class="bulleted-list"><li style="list-style-type:disc"><strong>penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’</strong><p id="1b44d778-1cdc-804b-ad5d-d7cbe5fa34f7" class="">Specify the norm of the penalty:<br/>• <code>None</code>: no penalty is added;<br/>• <code>&#x27;l2&#x27;</code>: add a L2 penalty term and it is the default choice;<br/>• <code>&#x27;l1&#x27;</code>: add a L1 penalty term;<br/>• <code>&#x27;elasticnet&#x27;</code>: both L1 and L2 penalty terms are added.</p></li></ul><ul id="1b44d778-1cdc-8098-ba48-f5f2a9a4bc38" class="bulleted-list"><li style="list-style-type:disc"><strong>solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’</strong><p id="1b44d778-1cdc-80a7-976d-c73090895b94" class="">Algorithm to use in the optimization problem. Default is ‘lbfgs’. </p><p id="1b44d778-1cdc-8086-bbc7-f0af0c5fb187" class="">For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;</p><p id="1b44d778-1cdc-8005-954d-de1c2e30e758" class="">For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;</p><p id="1b44d778-1cdc-805d-bd21-dbdf8b1cb211" class="">‘liblinear’ and ‘newton-cholesky’ can only handle binary classification by default. To apply a one-versus-rest scheme for the multiclass setting one can wrapt it with the <code>OneVsRestClassifier</code>.</p><p id="1b44d778-1cdc-803b-bb33-e912630cdc66" class="">‘newton-cholesky’ is a good choice for <code>n_samples</code> &gt;&gt; <code>n_features</code>, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on <code>n_features</code> because it explicitly computes the Hessian matrix.</p><figure id="1b44d778-1cdc-809e-a618-f3faf312621d" class="image"><a href="image%205.png"><img style="width:281.9940490722656px" src="image%205.png"/></a></figure><ul id="1b44d778-1cdc-8016-9d2f-c935999c033a" class="bulleted-list"><li style="list-style-type:circle">기타 등등<ul id="1b44d778-1cdc-8091-bd2c-d5a1acede3e1" class="bulleted-list"><li style="list-style-type:square">싸이킷런 공식 홈페이지에 들어가면 더 많은 하이퍼 파라미터를 확인할수 있습니다.</li></ul><ul id="1b44d778-1cdc-80f4-b9a7-d2aec011e75a" class="bulleted-list"><li style="list-style-type:square"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></li></ul></li></ul></li></ul></details></li></ul></details></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>