<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>BERT 와 fine-tuning</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.collection-content td {
	white-space: pre-wrap;
	word-break: break-word;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

blockquote.quote-large {
	font-size: 1.25em;
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.highlight-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.highlight-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.highlight-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.highlight-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.highlight-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.highlight-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.highlight-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.highlight-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.highlight-default_background {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray_background {
	background: rgba(240, 239, 237, 1);
}
.highlight-brown_background {
	background: rgba(245, 237, 233, 1);
}
.highlight-orange_background {
	background: rgba(251, 235, 222, 1);
}
.highlight-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.highlight-teal_background {
	background: rgba(232, 241, 236, 1);
}
.highlight-blue_background {
	background: rgba(232, 242, 250, 1);
}
.highlight-purple_background {
	background: rgba(243, 235, 249, 1);
}
.highlight-pink_background {
	background: rgba(250, 233, 241, 1);
}
.highlight-red_background {
	background: rgba(252, 233, 231, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.block-color-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.block-color-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.block-color-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.block-color-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.block-color-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.block-color-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.block-color-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.block-color-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(240, 239, 237, 1);
}
.block-color-brown_background {
	background: rgba(245, 237, 233, 1);
}
.block-color-orange_background {
	background: rgba(251, 235, 222, 1);
}
.block-color-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.block-color-teal_background {
	background: rgba(232, 241, 236, 1);
}
.block-color-blue_background {
	background: rgba(232, 242, 250, 1);
}
.block-color-purple_background {
	background: rgba(243, 235, 249, 1);
}
.block-color-pink_background {
	background: rgba(250, 233, 241, 1);
}
.block-color-red_background {
	background: rgba(252, 233, 231, 1);
}
.select-value-color-default { background-color: rgba(42, 28, 0, 0.07); }
.select-value-color-gray { background-color: rgba(28, 19, 1, 0.11); }
.select-value-color-brown { background-color: rgba(127, 51, 0, 0.156); }
.select-value-color-orange { background-color: rgba(196, 88, 0, 0.203); }
.select-value-color-yellow { background-color: rgba(209, 156, 0, 0.282); }
.select-value-color-green { background-color: rgba(0, 96, 38, 0.156); }
.select-value-color-blue { background-color: rgba(0, 99, 174, 0.172); }
.select-value-color-purple { background-color: rgba(92, 0, 163, 0.141); }
.select-value-color-pink { background-color: rgba(183, 0, 78, 0.152); }
.select-value-color-red { background-color: rgba(206, 24, 0, 0.164); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="2024d778-1cdc-803b-91de-c6cbf0551477" class="page sans"><header><h1 class="page-title">BERT 와 fine-tuning</h1><p class="page-description"></p></header><div class="page-body"><p id="2024d778-1cdc-80d4-8879-e7d715e209d6" class="">
</p><h3 id="8ea6ddfb-4365-48d0-b869-0b96a4366d06" class="">BERT의 등장 배경</h3><ul id="37c7cc04-f8b5-4c8d-8037-53995b9de4b3" class="bulleted-list"><li style="list-style-type:disc"><strong>사전 학습(Pre-training)과 미세 조정(Fine-tuning)의 패러다임</strong></li></ul><ul id="f0f3cd40-78bf-4364-a8bc-8cb796a1ec9f" class="bulleted-list"><li style="list-style-type:disc">기존 언어 모델의 한계 (단방향성)</li></ul><ul id="293d8bd8-ba82-4d52-ae60-ada94fb7679b" class="bulleted-list"><li style="list-style-type:disc">&quot;Attention is All You Need&quot; 논문의 영향</li></ul><h3 id="09c7304c-5949-4d3e-895f-6edf22d2f1d2" class="">BERT의 핵심 아이디어</h3><ul id="e54b93af-5312-480c-b41f-039fbee8923b" class="bulleted-list"><li style="list-style-type:disc">양방향(Bidirectional) 표현 학습의 중요성</li></ul><ul id="444eaf1b-9ac1-4019-a5f7-cadf055d61ba" class="bulleted-list"><li style="list-style-type:disc">문맥을 고려한 단어 표현</li></ul><ul id="c9dae41f-43ac-43ed-bbff-b2dfe68a9239" class="bulleted-list"><li style="list-style-type:disc">Transformer 인코더를 활용한 구조</li></ul><h3 id="5182aaa3-3c9d-4de4-a274-117aa07fcecb" class="">BERT의 사전 학습 방법</h3><ul id="eb2871a1-2e0c-4a9e-8126-410ec9a8d299" class="bulleted-list"><li style="list-style-type:disc">MLM(Masked Language Modeling)<ul id="e4adcf21-4d8d-47ae-a1c0-58bd81d34256" class="bulleted-list"><li style="list-style-type:circle">입력의 일부를 마스킹하고 원래 단어 예측</li></ul><ul id="03fc4c6b-293f-4b0b-af29-4e6f058268bc" class="bulleted-list"><li style="list-style-type:circle">양방향 문맥 활용의 핵심 기법</li></ul></li></ul><ul id="4456ed64-16f5-4cb1-a9e5-9a6b877e175a" class="bulleted-list"><li style="list-style-type:disc">NSP(Next Sentence Prediction)<ul id="a65a2d08-ab6a-48d1-9547-427ca0b9d8e3" class="bulleted-list"><li style="list-style-type:circle">두 문장의 연속성 학습</li></ul><ul id="f8c9f58a-9521-4b3e-853e-67b655a9364b" class="bulleted-list"><li style="list-style-type:circle">문서 수준의 이해 능력 향상</li></ul></li></ul><h3 id="4b3263e0-2826-4afe-b887-43e22bb8314a" class="">BERT의 응용 분야</h3><ul id="bc775fb9-6cde-4327-8933-fa3d93fecd87" class="bulleted-list"><li style="list-style-type:disc">텍스트 분류(감성 분석, 주제 분류 등)</li></ul><ul id="6fa42e4c-fd54-4e69-83e1-d3f0eaf99aeb" class="bulleted-list"><li style="list-style-type:disc">개체명 인식(Named Entity Recognition)</li></ul><ul id="35b1db01-66d8-4d6a-85e9-fb1acf5a8b7b" class="bulleted-list"><li style="list-style-type:disc">질의응답(Question Answering)</li></ul><ul id="8e61340b-9ed2-4c77-9e43-56772aee673d" class="bulleted-list"><li style="list-style-type:disc">자연어 추론(Natural Language Inference)</li></ul><ul id="12f72798-9c89-47ae-8999-44085adebd01" class="bulleted-list"><li style="list-style-type:disc">문장 쌍 분류(Sentence Pair Classification)</li></ul><h2 id="4dbf4e56-a26c-4c6a-b738-c9235a94c9ab" class="">학습 목표</h2><ol type="1" id="ac6cac17-c41a-4b8e-8e48-09074631dc32" class="numbered-list" start="1"><li>BERT의 사전 학습 방식과 응용 방법 습득</li></ol><ol type="1" id="4527af01-cf94-4a46-8195-fba28fee0678" class="numbered-list" start="2"><li>Transformer와 BERT의 차이점 및 각각의 장단점 이해</li></ol><p id="2024d778-1cdc-80f4-bce9-cd9b56a17596" class="">
</p><p id="2024d778-1cdc-807f-b59f-c28c387da9f5" class="">
</p><p id="2024d778-1cdc-8039-b140-d0439432c461" class="">
</p><hr id="2024d778-1cdc-800c-9c4d-c52b118a5b67"/><p id="2024d778-1cdc-8084-a408-eec64318f5f1" class="">
</p><h3 id="096ba56c-cc33-41db-8084-2a3665cdb531" class="">1. BERT의 구조</h3><p id="a6a52f8a-13a8-407d-95b5-210aec40666d" class="">BERT는 Transformer의 Encoder 부분만 사용하는 모델입니다.</p><p id="03b5026e-d44e-4ab0-9126-442d1aa8fed4" class="">모델 크기에 따라 두 가지 버전이 있습니다:</p><ul id="e8ac5059-b006-4e5e-833a-c7f2a67129fa" class="bulleted-list"><li style="list-style-type:disc"><strong>BERT-Base</strong>: 12개 레이어, 768 히든 크기, 12 어텐션 헤드 (110M 파라미터)</li></ul><ul id="29ebd36a-18b5-4bd3-a586-711aa7f920e4" class="bulleted-list"><li style="list-style-type:disc"><strong>BERT-Large</strong>: 24개 레이어, 1024 히든 크기, 16 어텐션 헤드 (340M 파라미터)</li></ul><p id="5382fcfc-9372-4f2e-8cb8-24ef56eaf475" class="">특징:</p><ul id="e9e200d0-eca7-44eb-9c08-3132f1a4a697" class="bulleted-list"><li style="list-style-type:disc">양방향(Bidirectional) 모델로, 문맥의 좌우 모든 정보 활용</li></ul><ul id="eaa9082a-ad2f-41f7-95ea-62837ece06b7" class="bulleted-list"><li style="list-style-type:disc">다양한 NLP 태스크에 적용 가능한 범용 모델</li></ul><ul id="8493f34f-df67-4f69-a73c-5372fefdc512" class="bulleted-list"><li style="list-style-type:disc">대규모 텍스트 데이터로 사전 학습 후 특정 태스크에 미세 조정</li></ul><p id="2024d778-1cdc-807d-829a-e1bb6dd35efd" class="">
</p><figure id="2024d778-1cdc-80ee-9056-e3af1fd1bf86" class="image"><a href="image%2080.png"><img style="width:709.96875px" src="image%2080.png"/></a></figure><p id="2024d778-1cdc-8023-a857-d7a94d44641a" class="">
</p><p id="2024d778-1cdc-8073-abbf-d4a326300772" class="">
</p><p id="2024d778-1cdc-80ab-bacd-c900bf6eb677" class="">
</p><p id="2024d778-1cdc-801f-877e-d98048d65707" class="">
</p><p id="2024d778-1cdc-800b-9066-c99ffd7ccc67" class="">
</p><h3 id="fbe75b84-a046-4344-ba2e-b28b0ba3c028" class="">2. 사전 학습 태스크</h3><h3 id="c92b8f3d-90ed-4f76-9810-51a48ac0ed8d" class="">Masked Language Model (MLM)</h3><ul id="1e922517-2cf2-4f2b-8150-9f9434d54347" class="bulleted-list"><li style="list-style-type:disc">입력 토큰의 15%를 무작위로 선택하여 마스킹 처리</li></ul><ul id="68c7ac0b-bc99-4b22-abf7-0c098a4114ca" class="bulleted-list"><li style="list-style-type:disc">마스킹된 토큰의 80%는 [MASK] 토큰으로 대체</li></ul><ul id="6245c1d4-186c-428b-b3cd-fcd8975f018e" class="bulleted-list"><li style="list-style-type:disc">10%는 무작위 단어로 대체</li></ul><ul id="4e4ae7c3-7def-4225-bcdc-26e01cab7300" class="bulleted-list"><li style="list-style-type:disc">10%는 원래 단어 그대로 유지</li></ul><ul id="f5ac356a-b1fd-423d-9d0e-2969dede9440" class="bulleted-list"><li style="list-style-type:disc">모델이 마스킹된 위치의 원래 단어를 예측하도록 학습</li></ul><p id="a74bec2f-89b1-458f-9898-20b97f7d4995" class="">이 방식을 통해 모델은 양방향 문맥을 고려하여 단어를 예측하는 능력을 갖게 됩니다.</p><figure id="2024d778-1cdc-80b4-ab9a-ebe09c6c5601" class="image"><a href="image%2081.png"><img style="width:521.9744262695312px" src="image%2081.png"/></a></figure><p id="2024d778-1cdc-8078-b94a-cc355704cf7c" class="">
</p><h3 id="662a3eae-f313-4b94-a404-12c6cff066c4" class="">Next Sentence Prediction (NSP)</h3><ul id="57fc6de9-2d1e-4493-8627-e5f5522b66c2" class="bulleted-list"><li style="list-style-type:disc">두 문장이 연속적인지 판단하는 이진 분류 태스크</li></ul><ul id="f3133469-9a4c-4208-b848-a14632c2433b" class="bulleted-list"><li style="list-style-type:disc">훈련 데이터는 다음과 같이 구성:<ul id="e8b5bf64-9981-4107-a2d1-a350eb9ec0fe" class="bulleted-list"><li style="list-style-type:circle">50%: 실제 연속된 두 문장(IsNext)</li></ul><ul id="fb44a0cb-6001-48d6-9247-b768e5ac2dc8" class="bulleted-list"><li style="list-style-type:circle">50%: 무관한 두 문장(NotNext)</li></ul></li></ul><ul id="e100e765-67d2-4e85-a592-7350da1fe69c" class="bulleted-list"><li style="list-style-type:disc">[CLS] 토큰의 표현을 사용해 분류</li></ul><ul id="eb50b1de-1758-40e3-b438-1427e9bee77a" class="bulleted-list"><li style="list-style-type:disc">문서 수준의 이해와 문장 간 관계 학습에 도움</li></ul><p id="2024d778-1cdc-80bb-806d-de380332c321" class="">
</p><p id="2024d778-1cdc-80ff-a7ef-f74c1ae179c4" class="">
</p><p id="2024d778-1cdc-8067-87f9-f1dde62420f8" class="">
</p><p id="2024d778-1cdc-8082-a9d4-e6b2d80b4609" class="">
</p><h3 id="d7533690-dcad-48fd-8b8c-a4c4d55c64c7" class="">3. 토큰화와 임베딩</h3><h3 id="a526191c-4a40-410b-9cab-abad475f9ab7" class="">WordPiece 토큰화</h3><ul id="5802c4bb-5edb-443b-926b-776349c4bdcf" class="bulleted-list"><li style="list-style-type:disc">단어를 하위 단위(subword)로 분할하는 방법</li></ul><ul id="b782e0fd-da82-450c-9dfe-ca226208e4dd" class="bulleted-list"><li style="list-style-type:disc">자주 등장하는 단어는 그대로, 희귀한 단어는 분할</li></ul><ul id="65b7b913-4aec-4172-a8da-aefcf64a844a" class="bulleted-list"><li style="list-style-type:disc">OOV(Out-of-Vocabulary) 문제 완화</li></ul><ul id="177907e9-5a43-47b4-b8a4-3b6d4462d1d4" class="bulleted-list"><li style="list-style-type:disc">예: playing → play + ##ing</li></ul><ul id="2024d778-1cdc-8064-b736-f89ca521f8f4" class="bulleted-list"><li style="list-style-type:disc">한국어의 경우에도 대부분 변화했음(2021년) - 대표 : klue/bert-base : KAIST 제작</li></ul><p id="2024d778-1cdc-800c-b8f2-ebc819608c3a" class="">
</p><h3 id="5f97f613-5e45-4cc6-ad63-2701a64f2662" class="">특수 토큰</h3><ul id="33feaaa1-0514-492b-9233-37c1cf3ca952" class="bulleted-list"><li style="list-style-type:disc">[CLS]: 분류 토큰, 시퀀스의 시작을 나타냄</li></ul><ul id="9a760768-b903-4cc4-a130-4098495ff07d" class="bulleted-list"><li style="list-style-type:disc">[SEP]: 구분 토큰, 문장 경계를 표시</li></ul><ul id="83382cb9-fc64-4b22-8817-41b9789a9a32" class="bulleted-list"><li style="list-style-type:disc">[MASK]: 마스킹된 토큰</li></ul><ul id="9a39ac93-8e61-43bc-a35e-ae0a97e753a6" class="bulleted-list"><li style="list-style-type:disc">[PAD]: 패딩 토큰</li></ul><p id="2024d778-1cdc-8080-a49a-d97c71e4c250" class="">
</p><h3 id="d1775b45-2e23-48c6-a5df-6d80fb522119" class="">입력 임베딩</h3><p id="4dd8c998-36a3-44a6-9fa8-f94e6ed11472" class="">BERT 입력은 세 가지 임베딩의 합으로 구성됩니다:</p><ul id="d3ddc758-40aa-43a2-8a4c-3f3e4cdff203" class="bulleted-list"><li style="list-style-type:disc"><strong>토큰 임베딩</strong>: 각 토큰의 의미 표현</li></ul><ul id="a350682a-05dd-4926-ac44-d9b18712dc53" class="bulleted-list"><li style="list-style-type:disc"><strong>세그먼트 임베딩</strong>: 어떤 문장에 속하는지 표시 (문장 A 또는 B)</li></ul><ul id="0dfd12a9-52bb-4a0f-b9ed-9a503edb5132" class="bulleted-list"><li style="list-style-type:disc"><strong>위치 임베딩</strong>: 토큰의 위치 정보</li></ul><p id="2024d778-1cdc-80f3-b92e-fc2848cae320" class="">
</p><p id="2024d778-1cdc-808c-8703-c75eb758fb9f" class="">
</p><p id="2024d778-1cdc-80f3-b10a-f370505acd79" class=""><strong>Fine-tuning</strong></p><figure id="2024d778-1cdc-8034-83ec-e5b752e68183" class="image"><a href="image%2082.png"><img style="width:710px" src="image%2082.png"/></a></figure><p id="2024d778-1cdc-803d-aa42-cd5ad154ddfd" class="">
</p><p id="2024d778-1cdc-8019-99d6-ead73abef9cd" class="">간단하게 실행가능</p><p id="2024d778-1cdc-8096-8c44-f204672285ba" class="">보통의 경우 하나의 output layer만을 추가하여 <strong>end-to-end</strong>로 <strong>fine-tuning</strong>이 가능합니다.<br/><br/></p><p id="2024d778-1cdc-8036-b207-e207cf208b71" class=""><strong>[Transformer 와 BERT 차이점 요약 정리]</strong></p><figure id="2024d778-1cdc-8048-a83f-f11f6ac527cd" class="image"><a href="image%2083.png"><img style="width:364.9940490722656px" src="image%2083.png"/></a></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>