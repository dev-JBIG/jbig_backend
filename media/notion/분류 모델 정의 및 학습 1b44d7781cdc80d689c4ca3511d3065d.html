<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>분류 모델 정의 및 학습</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(115, 114, 110, 1);
	fill: rgba(115, 114, 110, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(205, 60, 58, 1);
	fill: rgba(205, 60, 58, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(115, 114, 110, 1);
	fill: rgba(115, 114, 110, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(205, 60, 58, 1);
	fill: rgba(205, 60, 58, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-default { background-color: rgba(84, 72, 49, 0.08); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1b44d778-1cdc-80d6-89c4-ca3511d3065d" class="page sans"><header><h1 class="page-title">분류 모델 정의 및 학습</h1><p class="page-description"></p></header><div class="page-body"><h1 id="1b44d778-1cdc-80c2-8665-d2731f0581bd" class="">로지스틱 회귀</h1><ul id="1b44d778-1cdc-800e-9353-e299bc9c03e7" class="toggle"><li><details open=""><summary><mark class="highlight-default">개념</mark></summary><h1 id="1b44d778-1cdc-8071-ab52-faf224e6f788" class="">로지스틱 회귀란?</h1><p id="1b44d778-1cdc-80af-b184-c1bf14bd650a" class="">로지스틱 회귀 알고리즘은 2진 분류 모델로 사용되고 있다.</p><p id="1b44d778-1cdc-8023-840f-faed6af67862" class="">따라서 <strong>로지스틱 회귀(Logistic Regression)</strong>는 회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0과 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘이다.</p><p id="1b44d778-1cdc-806b-9d73-ed493d7db7bb" class="">스팸 메일 분류기 같은 예시를 생각하면 쉽다. 어떤 메일을 받았을 때 그것이 스팸일 확률이 0.5 이상이면 spam으로 분류하고, 확률이 0.5보다 작은 경우 ham으로 분류하는거다. 이렇게 데이터가 2개의 범주 중 하나에 속하도록 결정하는 것을 2<strong>진 분류(binary classification)</strong>라고 한다.</p><p id="1b44d778-1cdc-80bf-b53d-d901788af198" class="">로지스틱 회귀를 이해하려면 우선 선형 회귀(Linear Regression)에 대한 개념을 익혀야 한다.</p><p id="1b44d778-1cdc-809e-9d6b-c18fd6118cd2" class="">예를 들어 어떤 학생이 공부하는 시간에 따라 시험에 합격할 확률이 달라진다고 해보자. <strong>선형 회귀를 사용하면</strong> 아래와 같은 그림으로 나타낼 수 있다.</p><figure id="1b44d778-1cdc-8083-9078-cd45dc9834a7" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F2b87ad5d-bd8e-43b8-887e-dd8a76bb2ccb%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F2b87ad5d-bd8e-43b8-887e-dd8a76bb2ccb%2Fimage.png"/></a></figure><p id="1b44d778-1cdc-80f2-a24e-e82feb25c856" class="">공부한 시간이 적으면 시험에 통과 못하고, 공부한 시간이 많으면 시험에 통과한다는 식으로 설명할 수 있다. 그런데 이 회귀선을 자세히 살펴보면 <strong>확률이 음과 양의 방향으로 무한대까지 뻗어 간다.</strong> 말 그대로 &#x27;선&#x27;이라서.</p><p id="1b44d778-1cdc-8035-8050-d330e5f28689" class="">그래서 공부를 2시간도 안하면 시험에 통과할 확률이 0이 안된다. 이건 말이 안된다.</p><p id="1b44d778-1cdc-8095-b8ed-f171419ddfd2" class="">만약 <strong>로지스틱 회귀</strong>를 사용하면 아래와 같이 나타난다.</p><figure id="1b44d778-1cdc-809c-b48d-cd12f3ac5c43" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fe53ce299-c1d4-4c8c-852a-007489d18236%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fe53ce299-c1d4-4c8c-852a-007489d18236%2Fimage.png"/></a></figure><p id="1b44d778-1cdc-8013-b8c7-ee81dfa717ea" class="">시험에 합격할 확률이 0과 1 사이의 값으로 그려진다. 이제야 좀 납득이 간다.</p><p id="1b44d778-1cdc-8042-8989-c460c569342a" class="">
</p><h3 id="1b44d778-1cdc-8077-8f0e-f0b80b77f585" class=""><strong>Classification Threshold (임계값)</strong></h3><p id="1b44d778-1cdc-80f8-bda5-f01bc195b25e" class="">이렇게 로지스틱 회귀 알고리즘의 결과 값은 &#x27;분류 확률&#x27;이고, 그래서 이 확률이 특정 수준 이상 확보되면 샘플이 그 클래스에 속할지 말지 결정할 수 있다.</p><p id="1b44d778-1cdc-80fe-ba2a-c97f568d72fa" class="">그리고 당연히 대부분 알고리즘에서 기본 임계값은 <strong>0.5</strong>다.</p><p id="1b44d778-1cdc-8022-8f7c-fb15b47ca7bb" class="">다만, 필요에 따라 모델의 임계값을 변경할 수 있다. 예를 들어, 암을 진단하는 로지스틱 회귀 모델을 작성하는 경우에는 혹시 모를 경우에 대비하여 좀 더 민감하게 확인하기 위해 0.3이나 0.4로 임계값을 낮춰 모델의 민감도를 높일 필요가 있다. 그래야 전체적으로 오분류가 더 많아지더라도 실제 암 환자를 놓치는 사례는 적어질 테니까.</p><figure id="1b44d778-1cdc-8027-ba2a-c06a1859381f" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fa5475bf9-674a-4c9d-b2c6-e58479558e65%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2Fa5475bf9-674a-4c9d-b2c6-e58479558e65%2Fimage.png"/></a></figure><figure id="1b44d778-1cdc-80ea-8bb9-d1d207722b1a" class="image"><a href="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F4d304ab3-8a1d-4ef2-87a2-96065d6cf00c%2Fimage.png"><img src="https://velog.velcdn.com/images%2Fhyesoup%2Fpost%2F4d304ab3-8a1d-4ef2-87a2-96065d6cf00c%2Fimage.png"/></a></figure><figure id="1b94d778-1cdc-8050-ba31-fb3aa93a0191"><div class="source"><a href="https://youtu.be/m7oSyX4QCwY?si=WyhHvljtOxMgSKOp">https://youtu.be/m7oSyX4QCwY?si=WyhHvljtOxMgSKOp</a></div></figure><h2 id="1b44d778-1cdc-809f-87b7-ff090a4a6d1d" class="">요약</h2><ul id="1b44d778-1cdc-807c-8587-cb1315ec51a0" class="bulleted-list"><li style="list-style-type:disc">로지스틱 회귀 분석은 이진 분류를 수행하는 데 사용된다. 즉, 데이터 샘플을 양성(1) 또는 음성(0) 클래스 둘 중 어디에 속하는지 예측한다.</li></ul><ul id="1b44d778-1cdc-8036-a228-cde3d23bfc09" class="bulleted-list"><li style="list-style-type:disc">각 속성(feature)들의 계수 log-odds를 구한 후 sigmoid 함수를 적용하여 실제로 데이터가 해당 클래스에 속할 확률을 0과 1사이의 값으로 나타낸다.</li></ul><ul id="1b44d778-1cdc-80fd-959d-f6cc0b411e8a" class="bulleted-list"><li style="list-style-type:disc">데이터가 클래스에 속할지 말지 결정할 확률 컷오프를 Threshold(임계값)이라 한다. 기본 값은 0.5지만 데이터의 특성이나 상황에 따라 조정할 수 있다.</li></ul></details></li></ul><ul id="1b44d778-1cdc-8025-8f30-c1f55101a830" class="toggle"><li><details open=""><summary><mark class="highlight-default">로지스틱 회귀도 다중분류가 가능하다?</mark></summary><p id="1b44d778-1cdc-801c-b868-c798241210f6" class="">로지스틱회귀는 사실 클래스를 3개 이상으로 분류하는 다중분류(Multinomial Calssification)도 처리할 수 있습니다. </p><p id="1b44d778-1cdc-806e-9189-e744628304a4" class="">이진분류는 하나의 선형방정식과 하나의 확률값이 나와 이를 기준으로 데이터를 분류하는 것입니다.</p><p id="1b44d778-1cdc-80b6-8619-ce3efa992354" class="">그에 반해 다중 분류는 각 클래스별로 선형방정식이 나오고 그에 따른 확률값이 계산됩니다. </p><p id="1b44d778-1cdc-8064-9ac9-f86f1542a6af" class="">바꿔 말하면 3개 클래스로 예측하는 경우 각 데이터 샘플에 대해 3개의 선형방정식과 그에 따른 3개의 확률값이 나오고 그 중에 가장 큰 확률값을 가진 클래스로 예측되는 것입니다.</p><figure id="1b44d778-1cdc-8066-a408-e6e655c91cf9" class="image"><a href="image%208.png"><img style="width:337.98809814453125px" src="image%208.png"/></a></figure><ul id="1ba4d778-1cdc-80b1-9267-dc2e441fe36a" class="toggle"><li><details open=""><summary><mark class="highlight-default">이진분류와의 원리적 차이</mark></summary><p id="42e7ea38-f90f-4c30-94e9-2f3375046cd7" class="">그렇다면 하나의 확률값을 만들어 나머지 클래스는 자동으로 (1-확률값)이 되는 시그모이드가 아니라 각각의 선형방정식에서 나온 결과값들을 종합해 확률로 변환하는 함수가 필요합니다.</p><p id="cce86d06-ed0a-4ada-8e88-c822fec64329" class="">바로 소프트맥스(SoftMax) 함수인데 소프트맥스는 선형방정식에서 나온 결과값(z)을 자연상수를 밑으로 하는 지수함수의 지수로 사용하고 그 값들을 모두 더해 분모로 하고 각 결과값의 지수함수값을 분자로 그 비중에 따라 0-1 사이의 확률로 변환해줍니다.</p><figure id="024afedd-8b0b-4ca4-8c20-58c383db4e25" class="image"><a href="image%209.png"><img style="width:567.9921875px" src="image%209.png"/></a></figure><p id="69c6a03e-905a-4246-9b06-c6a0e4d31bf1" class="">z는 실수값의 입력 벡터</p><p id="312299e8-b520-4aa2-99a4-783b2745a3b8" class="">K는 분류 클래스 수</p><p id="ebab57a2-b7df-46b2-9b85-5c54c4d07eed" class="">e는 자연로그(오일러 수)의 밑</p><figure id="54c7fedc-9999-4b85-81ed-e0dea0180c24" class="image"><a href="image%2010.png"><img style="width:197.99444580078125px" src="image%2010.png"/></a></figure><h3 id="e5e6718b-4636-4d19-aab3-d560fe6b4aa8" class="">시그모이드와 소프트맥스의 차이점</h3><figure id="38843259-693a-45f9-93f5-742bcc705505" class="image"><a href="image%2011.png"><img style="width:567.9921875px" src="image%2011.png"/></a></figure><p id="b3501bb3-6bcd-42dd-827f-bdf95e3db472" class="">언뜻 보기에 시그모이드 함수와 소프트맥스 함수는 두 함수 모두 입력 값을 0과 1 사이의 숫자 범위에 매핑하기 때문에 비교적 비슷해 보입니다. </p><p id="9f12c042-b642-4d53-a44e-5367b4bca486" class="">또한 시그모이드 함수는 x = 0에서 0.5 값을 통과하고 소프트맥스 함수는 이 시점에서 여전히 0.5 미만이라는 차이점을 제외하고는 거의 동일한 과정을 거칩니다.</p><p id="6214f879-6a74-4a87-a5d6-cc3451a31208" class="">두 함수의 차이점은 적용에 있습니다. </p><p id="ca9bcb01-d585-4f90-9147-e5a3cd8f7276" class="">시그모이드 함수는 이진 분류, 즉 서로 다른 두 클래스 사이에서 결정을 내려야 하는 모델에 사용할 수 있습니다. </p><p id="eefe3f0f-c064-42e3-a4b5-34aa98ff34b9" class="">반면 소프트맥스는 두 개 이상의 클래스를 예측해야 하는 분류에도 사용할 수 있습니다. 이 경우 이 함수는 모든 클래스의 확률이 1이 되도록 보장합니다.</p><p id="d8b64262-ce9d-4bcd-8e62-1e381c9610d6" class="">실제로 두 클래스의 경우 시그모이드 함수와 소프트맥스 함수가 일치한다는 것을 수학적으로 증명할 수도 있습니다.</p><p id="e51bc1d5-f465-477f-9fa7-e4aab98a9851" class="">
</p><p id="17ad9803-449c-4f85-9d91-05e945af41ec" class="">시그모이드, 소프트맥스는 딥러닝에서도 중요한 역할을 하니 잘 알아두세요!!</p><h2 id="cdd10351-5fda-48ce-b382-634eac442a99" class="">간단 정리 !</h2><h3 id="96aa9279-c608-446e-ab6f-688396638747" class="">시그모이드 - 이진 분류 모델의 마지막 활성화 함수 !</h3><h3 id="6c2d9d70-d8fc-49e1-9e74-d45b3d22f149" class="">소프트맥스 - 다중 분류 모델의 마지막 활성화 함수 !</h3></details></li></ul></details></li></ul><ul id="1b44d778-1cdc-80e3-aab8-dc712989bc81" class="toggle"><li><details open=""><summary><mark class="highlight-default">코드</mark></summary><p id="1b44d778-1cdc-8022-bf0f-dcd45db1da57" class="">이진분류(Sigmoid)</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-809a-ba60-f807636483a2" class="code"><code class="language-Python">from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
data = load_iris()
X = data[&#x27;data&#x27;][:100]  # Iris setosa와 Iris versicolor만 선택
y = data[&#x27;target&#x27;][:100]

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 로지스틱 회귀 모델 생성 및 훈련
model = LogisticRegression()
model.fit(X_train, y_train)

# 예측 및 성능 평가
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f&#x27;Accuracy: {accuracy:.2f}&#x27;)
</code></pre><p id="1b44d778-1cdc-80b9-916c-f1fbc23d8f2e" class="">다중분류(SoftMax)</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-80f5-abe8-d35fc3ea49f9" class="code"><code class="language-Python">from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
data = load_iris()
X = data[&#x27;data&#x27;]
y = data[&#x27;target&#x27;]

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Softmax 회귀 모델 생성 및 훈련
model = LogisticRegression(multi_class=&#x27;multinomial&#x27;,max_iter=200)
model.fit(X_train, y_train)

# 예측 및 성능 평가
predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f&#x27;Accuracy: {accuracy:.2f}&#x27;)
</code></pre><p id="1b44d778-1cdc-80f5-b399-d2ea81f9f38b" class="">
</p><p id="1b44d778-1cdc-80c6-b65f-d17e743b54b0" class="">근데 사실 공식홈페이지를 확인해보니 auto가 default값이라 따로 설정안해줘도 되겠네요ㅎㅎ</p><figure id="1b44d778-1cdc-80be-b678-c34e49ccf914" class="image"><a href="image%2012.png"><img style="width:427px" src="image%2012.png"/></a></figure><p id="1b44d778-1cdc-80ee-a8be-c471a14fed3d" class="">
</p><ul id="1b44d778-1cdc-80bf-81b1-c5874f1e4998" class="toggle"><li><details open=""><summary>하이퍼 파라미터s</summary><ul id="1b44d778-1cdc-8079-bd62-ccae392762c5" class="bulleted-list"><li style="list-style-type:disc"><strong><span style="border-bottom:0.05em solid">하나하나 다 공부할 필요까지는 없고 읽어보면서 이런것들을 조정해서 모델 성능을 올릴수 있구나까지 생각해도 충분합니다.</span></strong></li></ul><ul id="1b44d778-1cdc-809d-97d4-ed3426f225f6" class="bulleted-list"><li style="list-style-type:disc"><strong>max_iterint, default=100</strong><p id="1b44d778-1cdc-8052-bed3-e40c8ad396e2" class="">Maximum number of iterations taken for the solvers to converge.</p></li></ul><ul id="1b44d778-1cdc-80f8-9072-c9c44b2ac785" class="bulleted-list"><li style="list-style-type:disc"><strong>penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’</strong><p id="1b44d778-1cdc-804b-ad5d-d7cbe5fa34f7" class="">Specify the norm of the penalty:<br/>• <br/><code>None</code>: no penalty is added;<br/>• <br/><code>&#x27;l2&#x27;</code>: add a L2 penalty term and it is the default choice;<br/>• <br/><code>&#x27;l1&#x27;</code>: add a L1 penalty term;<br/>• <br/><code>&#x27;elasticnet&#x27;</code>: both L1 and L2 penalty terms are added.</p></li></ul><ul id="1b44d778-1cdc-8098-ba48-f5f2a9a4bc38" class="bulleted-list"><li style="list-style-type:disc"><strong>solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’</strong><p id="1b44d778-1cdc-80a7-976d-c73090895b94" class="">Algorithm to use in the optimization problem. Default is ‘lbfgs’. </p><p id="1b44d778-1cdc-8086-bbc7-f0af0c5fb187" class="">For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;</p><p id="1b44d778-1cdc-8005-954d-de1c2e30e758" class="">For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;</p><p id="1b44d778-1cdc-805d-bd21-dbdf8b1cb211" class="">‘liblinear’ and ‘newton-cholesky’ can only handle binary classification by default. To apply a one-versus-rest scheme for the multiclass setting one can wrapt it with the <code>OneVsRestClassifier</code>.</p><p id="1b44d778-1cdc-803b-bb33-e912630cdc66" class="">‘newton-cholesky’ is a good choice for <code>n_samples</code> &gt;&gt; <code>n_features</code>, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on <code>n_features</code> because it explicitly computes the Hessian matrix.</p><figure id="1b44d778-1cdc-809e-a618-f3faf312621d" class="image"><a href="image%2013.png"><img style="width:281.9940490722656px" src="image%2013.png"/></a></figure><ul id="1b44d778-1cdc-8016-9d2f-c935999c033a" class="bulleted-list"><li style="list-style-type:circle">기타 등등<ul id="1b44d778-1cdc-8091-bd2c-d5a1acede3e1" class="bulleted-list"><li style="list-style-type:square">싸이킷런 공식 홈페이지에 들어가면 더 많은 하이퍼 파라미터를 확인할수 있습니다.</li></ul><ul id="1b44d778-1cdc-80f4-b9a7-d2aec011e75a" class="bulleted-list"><li style="list-style-type:square"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></li></ul></li></ul></li></ul></details></li></ul></details></li></ul><h1 id="1b44d778-1cdc-808a-8878-dfa6cda972a5" class="">의사결정나무(Decision Tree)</h1><ul id="1b44d778-1cdc-80e9-81f7-f2d324277548" class="toggle"><li><details open=""><summary><mark class="highlight-default">의사결정나무(Decision Tree)</mark></summary><p id="1b44d778-1cdc-808d-abb4-ce857a551337" class="">의사결정나무는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내며, 그 모양이 ‘나무’와 같다고 해서 의사결정나무라 불립니다. 질문을 던져서 대상을 좁혀나가는 <strong>‘스무고개’</strong> 놀이와 비슷한 개념입니다. </p><p id="1b44d778-1cdc-805e-a0fc-cbe6ab46708a" class="">
</p><p id="1b44d778-1cdc-80e6-97c0-f811dab0b882" class="">사진에서 보이는 판단기준 하나를 “Node”라고 부릅니다. 또한 마지막 최종 결과를 “Leaf”라고 합니다. 노드들의 판단기준으로 리프라는 결과가 나온다고 보시면 됩니다.</p><figure id="1b44d778-1cdc-80f0-9d54-d65f3a883c55" class="image"><a href="image%2014.png"><img style="width:388.9921875px" src="image%2014.png"/></a></figure><p id="1b44d778-1cdc-8021-8539-f6647dbb698c" class="">단순 일차함수  <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">y = \beta_0 + \beta_1x +\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span><span>﻿</span></span>를 생각해봅시다.</p><figure id="1b44d778-1cdc-808d-852f-d5f8f48713a8" class="image"><a href="Untitled%202.png"><img style="width:595.9921875px" src="Untitled%202.png"/></a></figure><p id="1b44d778-1cdc-807c-87f7-e9190deae868" class="">주어진 관측치를 바탕으로, 다음 관측치가 주어졌을 때, 그에 대한 반응변수를 예측하기 위한 회귀 모형을 의사결정나무( Decision Tree)를 이용하여 생성한다고 해봅시다.</p><figure id="1b44d778-1cdc-8046-bf99-f763f08aa13d" class="image"><a href="Untitled%203.png"><img style="width:595.9921875px" src="Untitled%203.png"/></a></figure><p id="1b44d778-1cdc-8065-90e2-ff351d567dc6" class="">첫번째 step에서 <strong><span style="border-bottom:0.05em solid">어떠한 방식</span></strong>으로 인해  <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">x=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span></span><span>﻿</span></span>가 기준이 되었고, 위 같은 직선이 생성되었습니다.</p><figure id="1b44d778-1cdc-802a-97fe-eb1242ba328b" class="image"><a href="Untitled%204.png"><img style="width:595.9921875px" src="Untitled%204.png"/></a></figure><p id="1b44d778-1cdc-8015-9b81-fef4d83ff5c8" class="">두 번째 step에서도 <strong><span style="border-bottom:0.05em solid">어떠한 방식</span></strong>으로 인해, 기준이 나누어졌고, 위와 같은 직선이 생성되었습니다.</p><figure id="1b44d778-1cdc-805a-b476-c8b42519fd8f" class="image"><a href="Untitled%205.png"><img style="width:427px" src="Untitled%205.png"/></a></figure><p id="1b44d778-1cdc-8055-80e5-e2cd2ce53000" class="">여러 step을 거쳐, 위와 같이 관측치를 예측하는 선이 생성되었습니다.</p><p id="1b44d778-1cdc-80a9-831e-f4633a152bd6" class="">( 위 사진은 의사결정나무가 Overfitting에 취약하다는 단점을 보여주는 사진이기도 합니다.)</p><p id="1b44d778-1cdc-808a-9ff9-c1f55aae7799" class="">이는 어떻게 생성이 되는 것일까요? 앞서 언급한 <strong><span style="border-bottom:0.05em solid">어떠한 방식</span></strong>, 즉, 분할 기준은 어떻게 정하는지에 대한 과정을 알아보겠습니다.</p><ol type="1" id="1b44d778-1cdc-8079-81ef-f9d1103a279f" class="numbered-list" start="1"><li>적당한 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span><span>﻿</span></span>을 고른다.</li></ol><ol type="1" id="1b44d778-1cdc-80d9-827b-e7d5d90f8d71" class="numbered-list" start="2"><li>분할 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>−</mo><mi mathvariant="normal">∞</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">[</mo><mi>c</mi><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(-\infin, c), [c, \infin)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">∞</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">[</span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∞</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span>에 대해 y의 예측값을 계산한다.</li></ol><ol type="1" id="1b44d778-1cdc-8045-ab3d-f9d60535ee98" class="numbered-list" start="3"><li>y의 예측값과 정답 y의 loss(r2 score)를 계산한다.</li></ol><ol type="1" id="1b44d778-1cdc-8048-820d-d367abce7edf" class="numbered-list" start="4"><li>1-3의 과정을 반복하여 loss가 가장 적은 c가 무엇인지 찾는다.</li></ol><figure id="1b44d778-1cdc-8025-af0d-e90e63c925a2" class="image"><a href="Untitled%206.png"><img style="width:427px" src="Untitled%206.png"/></a></figure><p id="1b44d778-1cdc-80fd-9d91-e4c6b4172f82" class="">
</p><p id="1b44d778-1cdc-802a-b9f4-e2144d9c7680" class="">
</p><p id="1b44d778-1cdc-8057-9e5c-feb564e8c5d1" class=""><strong>의사결정나무(개별 tree)의 단점</strong></p><ul id="1b44d778-1cdc-8016-a4f9-fd28aa74ac85" class="bulleted-list"><li style="list-style-type:disc">계층적 구조로 인해 중간에 에러가 발생하면 다음 단계에도 에러가 계속 전파됩니다.</li></ul><ul id="1b44d778-1cdc-8090-9721-fb514512864b" class="bulleted-list"><li style="list-style-type:disc">학습 데이터의 미세한 변동에도 최종 결과에 크게 영향을 끼칠 수 있습니다. (Overfitting)</li></ul><ul id="1b44d778-1cdc-8041-9c99-c4ebe57fb24a" class="bulleted-list"><li style="list-style-type:disc">outlier(이상치)에 큰 영향을 받습니다.</li></ul><ul id="1b44d778-1cdc-80b3-b141-c0bf89f0610a" class="bulleted-list"><li style="list-style-type:disc">나무의 최종 노드의 개수를 늘리면 overfitting(과적합) 위험이 발생합니다.</li></ul><p id="1b44d778-1cdc-8080-95a6-c3ca99dbfb08" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⇒</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">⇒</span></span></span></span></span><span>﻿</span></span> 해결방법 : 랜덤포레스트( <strong>Random Forest</strong>)</p><p id="1b44d778-1cdc-8097-8fc5-c0f1eac42982" class="">
</p></details></li></ul><h1 id="1b44d778-1cdc-80cd-836e-e8fe8b3e1426" class="">랜덤 포레스트</h1><ul id="1b44d778-1cdc-801a-b966-e82fe240c1b0" class="toggle"><li><details open=""><summary><mark class="highlight-default">랜덤 포레스트의 개념</mark></summary><h3 id="1b44d778-1cdc-80ab-95d5-e4ad135fe74c" class=""><strong>의사결정나무(개별 tree)의 단점</strong></h3><ul id="1b44d778-1cdc-8032-9d4d-d21217279140" class="bulleted-list"><li style="list-style-type:disc">계층적 구조로 인해 중간에 에러가 발생하면 다음 단계에도 에러가 계속 전파됩니다.</li></ul><ul id="1b44d778-1cdc-80c6-87d3-e1c24bca8cf8" class="bulleted-list"><li style="list-style-type:disc">학습 데이터의 미세한 변동에도 최종 결과에 크게 영향을 끼칠 수 있습니다. (Overfitting)</li></ul><ul id="1b44d778-1cdc-80a5-bb1d-f1ced3d3c180" class="bulleted-list"><li style="list-style-type:disc">outlier(이상치)에 큰 영향을 받습니다.</li></ul><ul id="1b44d778-1cdc-8003-bc4a-f8e8555fdf6e" class="bulleted-list"><li style="list-style-type:disc">나무의 최종 노드의 개수를 늘리면 overfitting(과적합) 위험이 발생합니다.</li></ul><p id="1b44d778-1cdc-8085-abfe-f83d822ea17c" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⇒</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">⇒</span></span></span></span></span><span>﻿</span></span> 해결방법 : 랜덤포레스트( <strong>Random Forest</strong>)</p><p id="1b44d778-1cdc-80cf-8cfd-c9d3b1080154" class="">
</p><h3 id="1b44d778-1cdc-80ba-9c49-dd258675059f" class=""><strong>랜덤포레스트( Random Forest)란?</strong></h3><ul id="1b44d778-1cdc-8029-b0a9-cad22652df2d" class="bulleted-list"><li style="list-style-type:disc">먼저, 랜덤포레스트의 구조를 시각화하면 다음과 같습니다.</li></ul><figure id="1b44d778-1cdc-8013-b53f-e6b2ff97bf31" class="image"><a href="Untitled%207.png"><img style="width:427px" src="Untitled%207.png"/></a></figure><ul id="1b44d778-1cdc-8023-9e5e-f6e3dcb239b7" class="bulleted-list"><li style="list-style-type:disc">여러 개별 tree( 의사결정나무)를 생성하여 각각 예측하게 한 뒤에, 각각의 예측 결과를 <strong>평균</strong> 혹은 <strong>다수결 법칙 투표(majority voting)</strong>를 이용하여 예측의 정확성을 향상시키는 모델</li></ul><ul id="1b44d778-1cdc-8086-a760-dee233ff1907" class="bulleted-list"><li style="list-style-type:disc">여기서 tree들은 서로 독립이고, 무작위 예측보다 성능이 좋아야한다는 전제조건이 필요합니다. 서로 독립이라는 의미는 각각의 tree는 서로 영향을 주고 받지 않고, 독립적으로 훈련을 한다는 것을 의미합니다.</li></ul><p id="1b44d778-1cdc-8019-9cc2-fd5fd6954d71" class="">
</p></details></li></ul><ul id="1b44d778-1cdc-8067-ae89-f932d220125c" class="toggle"><li><details open=""><summary><mark class="highlight-default">코드</mark></summary><p id="1b44d778-1cdc-80a0-9498-d3f80d94c57b" class="">다음은 scikit learn document에 기재되어 있는 예시코드입니다.</p><ul id="1b44d778-1cdc-808b-9090-fbdfa2b990f5" class="bulleted-list"><li style="list-style-type:disc">Classification</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-80d3-9584-de9f98fcba5e" class="code"><code class="language-Python">## Classification ##
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

X,y = make_classification(n_samples=1000, n_features=4,
													n_informative=2, n_redundant=0,
													random_state=0, shuffle=False)
													
clf = RandomForestClassifier(max_depth=2, random_state=0)

clf.fit(X,y)
clf.score(X,y)
#clf.predict(X_test)</code></pre><ul id="1b44d778-1cdc-8053-8dc0-d6b70288bd8d" class="bulleted-list"><li style="list-style-type:disc">Regression</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-804b-b9c7-ff6338078ea2" class="code"><code class="language-Python">## Regressor ##
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

X,y = make_regression(n_features=4, n_informative=2,
											random_state=0, shuffle=False)
											
regr = RandomForestRegressor(max_depth=2, random_state=0)

regr.fit(X,y)
regr.score(X,y)
# regr.predict(X_test)</code></pre><p id="1b44d778-1cdc-8050-a510-e477e1daac64" class="">
</p><ul id="1b44d778-1cdc-809e-aaa1-ccd7151fee40" class="toggle"><li><details open=""><summary>하이퍼 파라미터s</summary><ul id="1b44d778-1cdc-80f9-ad68-cc1c719d1910" class="bulleted-list"><li style="list-style-type:disc"><strong><span style="border-bottom:0.05em solid">하나하나 다 공부할 필요까지는 없고 읽어보면서 이런것들을 조정해서 모델 성능을 올릴수 있구나까지 생각해도 충분합니다.</span></strong></li></ul><ul id="1b44d778-1cdc-8087-97f5-cd40b0a2bcb4" class="bulleted-list"><li style="list-style-type:disc"><strong>n_estimators</strong>: 생성할 <span style="border-bottom:0.05em solid">트리의 개수</span> <strong>(int, default=100)</strong><ul id="1b44d778-1cdc-8051-9dea-ec52f0333a44" class="bulleted-list"><li style="list-style-type:circle">트리의 개수가 많을수록 앙상블 모델의 성능이 향상될 수 있지만, 계산 비용도 증가</li></ul><ul id="1b44d778-1cdc-809f-be2f-fa44966ecad0" class="bulleted-list"><li style="list-style-type:circle">트리 개수가 일정 수 이상이면 결과의 향상이 크게 나타나지 않음</li></ul></li></ul><ul id="1b44d778-1cdc-808f-85af-e8acfe714044" class="bulleted-list"><li style="list-style-type:disc"><strong>max_depth</strong>:  <span style="border-bottom:0.05em solid">트리의 최대 깊이</span> (<strong>int, default=None)</strong><ul id="1b44d778-1cdc-80e5-bed9-ffe867cb8914" class="bulleted-list"><li style="list-style-type:circle">깊이가 제한된 트리는 각각의 결정 경계를 더 간단하게 만들며, 모델이 데이터에 과적합되는 것을 방지할 수 있습니다.</li></ul><ul id="1b44d778-1cdc-802d-99bb-e05539837834" class="bulleted-list"><li style="list-style-type:circle">값을 작게 설정하면 모델의 단순성을 증가, 분산 감소, 편향 증가 → 데이터의 다양한 패턴을 잡아내지 못함, 일반화 능력 향상</li></ul><ul id="1b44d778-1cdc-8086-ad84-ed938f88ef03" class="bulleted-list"><li style="list-style-type:circle">값을 증가시키면 모델의 복잡성, 분산 증가, 편향 감소 → 과적합 가능성, BUT 더 복잡한 패턴을 학습 가능.</li></ul></li></ul><ul id="1b44d778-1cdc-8034-a24c-d33130054481" class="bulleted-list"><li style="list-style-type:disc"><strong>min_samples_split</strong>: 노드를 분할하기 위해 필요한 최소한의 샘플 개수 (<strong>int or float, default=2)</strong><ul id="1b44d778-1cdc-8018-a4a7-d767ca3a338f" class="bulleted-list"><li style="list-style-type:circle">샘플 개수가 이 값보다 작아지면 더 이상 분할하지 않고 리프 노드(말단 노드)로 설정</li></ul><ul id="1b44d778-1cdc-8058-b295-c3f96906f940" class="bulleted-list"><li style="list-style-type:circle">값을 작게 설정하면 모델 복잡성 증가. 더 복잡한 결정 경계를 가진 모델이 생성되어 데이터의 작은 변동에도 민감하게 반응 → 과적합 위험, 적은 수의 샘플을 기반으로 한 결정이 노이즈에 영향을 받을 가능성</li></ul><ul id="1b44d778-1cdc-80ee-b27f-cad089f902c4" class="bulleted-list"><li style="list-style-type:circle">값을 증가시키면 모델의 편향이 증가, 분산은 감소. 즉, 더 단순한 모델이 생성되어 모델의 복잡도 감소. 이는 결정 경계가 더 단순해지고 일부 정보의 손실이 발생 → 모델이 과적합되는 위험 감소, 너무 큰 값으로 설정하면 모델이 너무 단순해져서 예측 성능이 저하</li></ul></li></ul><ul id="1b44d778-1cdc-8006-a4b1-e11727401873" class="bulleted-list"><li style="list-style-type:disc"><strong>min_samples_leaf</strong>: 노드를 분할하기 위해 <span style="border-bottom:0.05em solid">리프 노드가 가져야 할 최소한의 샘플 개수</span> <strong>(int or float, default=1)</strong><ul id="1b44d778-1cdc-80e3-be55-dc60107e0e2b" class="bulleted-list"><li style="list-style-type:circle">값을 작게 설정하면 모델 복잡성 증가. → 과적합의 위험</li></ul><ul id="1b44d778-1cdc-8067-bc61-edba1c19de88" class="bulleted-list"><li style="list-style-type:circle">값을 증가시키면 모델의 편향은 증가하고 분산은 감소. 모델의 복잡도가 낮아지지만, 일부 정보의 손실이 발생</li></ul></li></ul><ul id="1b44d778-1cdc-8056-8f24-ecc5eaa72227" class="bulleted-list"><li style="list-style-type:disc"><strong>max_leaf_nodes</strong>: 생성될 수 있는<span style="border-bottom:0.05em solid">최대 리프 노드의 개수를 제한</span> <strong>(int, default=None)</strong><ul id="1b44d778-1cdc-8086-9329-ce8cad5ab113" class="bulleted-list"><li style="list-style-type:circle">일반적으로 랜덤 포레스트의 트리는 자라다가 가능한 한 많은 리프 노드를 생성하려는 경향이 있습니다.</li></ul><ul id="1b44d778-1cdc-8014-a2b1-d5ce9217d452" class="bulleted-list"><li style="list-style-type:circle">그러나 max_leaf_nodes를 설정함으로써 트리의 성장을 제한할 수 있습니다.</li></ul><ul id="1b44d778-1cdc-8059-9d6f-ce58237d3a69" class="bulleted-list"><li style="list-style-type:circle">max_leaf_nodes를 지정하면 트리는 해당 개수 이상의 리프 노드를 생성하지 않습니다.</li></ul><ul id="1b44d778-1cdc-80b9-ba66-efce73e138a1" class="bulleted-list"><li style="list-style-type:circle">이를 통해 과적합을 방지하고, 일반화 능력을 향상시킬 수 있습니다.</li></ul></li></ul><ul id="1b44d778-1cdc-80db-a060-e46982d44ab9" class="bulleted-list"><li style="list-style-type:disc">기타 등등<ul id="1b44d778-1cdc-8082-b08d-c886df08b015" class="bulleted-list"><li style="list-style-type:circle">싸이킷런 공식 홈페이지에 들어가면 더 많은 하이퍼 파라미터를 확인할수 있습니다.</li></ul><ul id="1b44d778-1cdc-802f-a3d0-efc619695d7a" class="bulleted-list"><li style="list-style-type:circle"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li></ul><p id="1b44d778-1cdc-8049-b4cb-c38412035d5b" class="">
</p></li></ul></details></li></ul></details></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>