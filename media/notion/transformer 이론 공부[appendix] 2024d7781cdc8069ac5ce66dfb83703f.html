<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>transformer 이론 공부[appendix]</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.collection-content td {
	white-space: pre-wrap;
	word-break: break-word;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

blockquote.quote-large {
	font-size: 1.25em;
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.highlight-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.highlight-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.highlight-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.highlight-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.highlight-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.highlight-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.highlight-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.highlight-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.highlight-default_background {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray_background {
	background: rgba(240, 239, 237, 1);
}
.highlight-brown_background {
	background: rgba(245, 237, 233, 1);
}
.highlight-orange_background {
	background: rgba(251, 235, 222, 1);
}
.highlight-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.highlight-teal_background {
	background: rgba(232, 241, 236, 1);
}
.highlight-blue_background {
	background: rgba(232, 242, 250, 1);
}
.highlight-purple_background {
	background: rgba(243, 235, 249, 1);
}
.highlight-pink_background {
	background: rgba(250, 233, 241, 1);
}
.highlight-red_background {
	background: rgba(252, 233, 231, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.block-color-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.block-color-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.block-color-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.block-color-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.block-color-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.block-color-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.block-color-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.block-color-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(240, 239, 237, 1);
}
.block-color-brown_background {
	background: rgba(245, 237, 233, 1);
}
.block-color-orange_background {
	background: rgba(251, 235, 222, 1);
}
.block-color-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.block-color-teal_background {
	background: rgba(232, 241, 236, 1);
}
.block-color-blue_background {
	background: rgba(232, 242, 250, 1);
}
.block-color-purple_background {
	background: rgba(243, 235, 249, 1);
}
.block-color-pink_background {
	background: rgba(250, 233, 241, 1);
}
.block-color-red_background {
	background: rgba(252, 233, 231, 1);
}
.select-value-color-default { background-color: rgba(42, 28, 0, 0.07); }
.select-value-color-gray { background-color: rgba(28, 19, 1, 0.11); }
.select-value-color-brown { background-color: rgba(127, 51, 0, 0.156); }
.select-value-color-orange { background-color: rgba(196, 88, 0, 0.203); }
.select-value-color-yellow { background-color: rgba(209, 156, 0, 0.282); }
.select-value-color-green { background-color: rgba(0, 96, 38, 0.156); }
.select-value-color-blue { background-color: rgba(0, 99, 174, 0.172); }
.select-value-color-purple { background-color: rgba(92, 0, 163, 0.141); }
.select-value-color-pink { background-color: rgba(183, 0, 78, 0.152); }
.select-value-color-red { background-color: rgba(206, 24, 0, 0.164); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="2024d778-1cdc-8069-ac5c-e66dfb83703f" class="page sans"><header><h1 class="page-title">transformer 이론 공부[appendix]</h1><p class="page-description"></p></header><div class="page-body"><p id="2024d778-1cdc-80d5-a9db-cfd5ab1b7981" class="">
</p><p id="2024d778-1cdc-8069-9c15-ca5462a79631" class="">
</p><p id="c2098862-4185-4b28-8341-e7b0278a0841" class="">Transformer는 크게 두 부분으로 구성됩니다:</p><ul id="90c8a0f4-1834-474a-b805-8ee7e6f78b07" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder</strong>: 입력 시퀀스를 벡터 표현(representation)으로 변환<ul id="6a4e4025-9170-41a7-8b46-a1fadf7d6de6" class="bulleted-list"><li style="list-style-type:circle">여러 개의 동일한 레이어를 쌓은 형태</li></ul><ul id="1863429d-2341-488f-9117-834e1b3be55d" class="bulleted-list"><li style="list-style-type:circle">각 레이어는 Self-Attention과 Feed-Forward Network로 구성</li></ul></li></ul><ul id="8371d6f4-931c-41d9-944b-4afd25821d0d" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder</strong>: 인코더의 출력 표현을 활용해 출력 시퀀스 생성<ul id="e3cbf9b9-e092-41a9-9a6f-ad6678c03c1f" class="bulleted-list"><li style="list-style-type:circle">인코더와 유사하게 여러 레이어로 구성</li></ul><ul id="aaa60505-7bc7-4853-92da-0233a7389894" class="bulleted-list"><li style="list-style-type:circle">Self-Attention, Encoder-Decoder Attention, Feed-Forward Network 포함</li></ul></li></ul><p id="2024d778-1cdc-8018-8ee9-fbed44205922" class="">
</p><p id="2024d778-1cdc-805f-a7cb-cf8525922eaf" class="">
</p><h3 id="486cba40-9011-4206-9e00-26d8dd8e870a" class="">2. 핵심 구성 요소</h3><h3 id="09ca0530-bb3d-40fe-a957-40110052495f" class="">Multi-Head Attention</h3><p id="9726af4b-c987-4cea-a6eb-412918189a23" class="">여러 개의 Attention 메커니즘을 병렬로 수행하여 다양한 관점에서 정보를 처리합니다.</p><p id="d53733b3-085a-4c58-af6a-22e8d8e2a0d1" class="">계산 과정:</p><ol type="1" id="7a196292-10b3-410f-8dd7-2f4890a10fab" class="numbered-list" start="1"><li>입력을 여러 개의 헤드로 분할</li></ol><ol type="1" id="1e36e637-2b30-4d70-9a80-b197f5f4ecbf" class="numbered-list" start="2"><li>각 헤드에서 독립적으로 Attention 계산</li></ol><ol type="1" id="c606e4ad-cb07-41b8-801b-b8b1e37b7ac0" class="numbered-list" start="3"><li>모든 헤드의 출력을 연결(concatenate)</li></ol><ol type="1" id="5d34e71d-b08e-435e-962c-bd028d56cec4" class="numbered-list" start="4"><li>선형 변환을 통해 최종 출력 생성</li></ol><p id="2efeff82-4f7c-4590-b1d6-bf66858d4629" class="">수식: Multi-Head(Q, K, V) = Concat(head₁, head₂, ..., headₕ)W^O</p><p id="6bff8cfb-ffa2-439f-babb-80a915a02b3f" class="">여기서 각 headᵢ = Attention(QW^Q_i, KW^K_i, VW^V_i)</p><p id="2024d778-1cdc-8062-bf10-df427b509f2b" class="">
</p><figure id="2024d778-1cdc-8098-9458-d322b94cfeb2" class="image"><a href="image%2075.png"><img style="width:710px" src="image%2075.png"/></a></figure><p id="2024d778-1cdc-808e-aa45-e7e4129128a1" class="">
</p><p id="2024d778-1cdc-8071-8a2c-c03700814f24" class="">scaled dot product attention </p><p id="2024d778-1cdc-805f-9607-fd9a54357dc7" class="">
</p><p id="2024d778-1cdc-808c-8bc4-fa088e07acf4" class="">
</p><h3 id="332e24f3-610b-4551-9628-2c6226acfd4f" class="">Position-wise Feed-Forward Networks</h3><p id="75e193f4-66ca-471b-af85-2abc435ba167" class="">각 위치별로 독립적으로 적용되는 2층 신경망입니다.</p><p id="5bcd3425-527f-4a37-943f-1883bcc17da3" class="">수식: FFN(x) = max(0, xW₁ + b₁)W₂ + b₂</p><p id="7017b467-10cb-44f4-a43d-2993d4590f04" class="">첫 번째 레이어는 확장(expansion)을 수행하고, 두 번째 레이어는 원래 차원으로 투영(projection)합니다.</p><p id="2024d778-1cdc-803b-adf2-c79077d90f87" class="">
</p><figure id="2024d778-1cdc-8078-bf9b-dcb30001ee41" class="image"><a href="image%2076.png"><img style="width:709.96875px" src="image%2076.png"/></a></figure><p id="2024d778-1cdc-8005-9d5b-e2eb39c51a5f" class="">
</p><p id="2024d778-1cdc-8057-8169-f7f60406bac5" class="">
</p><p id="2024d778-1cdc-80ae-983b-ed69a3287551" class="">
</p><h3 id="7554af25-52a1-4d1b-b751-a8303c17a9ca" class="">Layer Normalization</h3><p id="71871b89-bbde-4a9f-945c-3b104d085ce7" class="">각 서브레이어의 출력을 정규화하여 학습을 안정화합니다.</p><p id="72440f66-6d20-4f19-b3df-0fc76a20b202" class="">수식: LayerNorm(x) = α ⊙ (x - μ) / (σ² + ε)^(1/2) + β</p><p id="06c6cbad-b5f2-4b3c-8639-f606de9b6ba7" class="">여기서 μ와 σ는 각각 평균과 표준편차, α와 β는 학습 가능한 파라미터입니다.</p><p id="2024d778-1cdc-8085-b81d-c7c3714dbd27" class="">
</p><p id="2024d778-1cdc-8081-9e6b-f2fc79946a5a" class="">보통 normalization 은 channel 별로 진행한다. 그러나 transformer 는 layer 마다 normalization 을 진행하여 안정화시킨 방식이다.</p><p id="2024d778-1cdc-80c1-9551-e918deeed37a" class="">
</p><h3 id="8c3127c3-79a8-4d79-a05c-7407cb20ab0f" class="">3. Positional Encoding</h3><p id="4e3cd82b-5654-4d42-aa25-581c5e7b06bf" class="">Transformer는 단어의 순서 정보를 자연스럽게 처리하지 못하므로, 위치 정보를 명시적으로 추가합니다.</p><p id="5721ee10-7bb8-431c-b84c-c60def515f34" class="">수식:</p><ul id="aceefe35-9a52-4581-aa04-965e514042ff" class="bulleted-list"><li style="list-style-type:disc">PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</li></ul><ul id="d368cbf4-87d7-4a2d-bf13-2b3264e7cc18" class="bulleted-list"><li style="list-style-type:disc">PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</li></ul><p id="135e41f3-7d1b-4be6-a30c-0b14805adc87" class="">여기서 pos는 위치, i는 차원을 의미.</p><p id="2024d778-1cdc-8040-91f9-f876f44ba82a" class="">
</p><p id="2024d778-1cdc-802e-93f9-c5f20a07ec9f" class="">
</p><figure id="2024d778-1cdc-802a-9b6c-facb07555343" class="image"><a href="image%2077.png"><img style="width:709.96875px" src="image%2077.png"/></a></figure><p id="2024d778-1cdc-8094-bc83-dbfd1a5b7a20" class=""><br/>포지셔널 인코딩(Positional Encoding)에서 보이는 이미지에서 &#x27;depth&#x27;는 <strong>임베딩 벡터의 차원(dimension)</strong> 을 의미</p><p id="154ef154-724e-49ec-9ab0-1b86da1b2603" class="">Transformer 모델에서 포지셔널 인코딩은 시퀀스 내 토큰의 위치 정보를 모델에 주입하기 위해 사용됩니다. 이미지에서:</p><ul id="60ec0f76-6bb4-4bbc-b063-29de94c9caa0" class="bulleted-list"><li style="list-style-type:disc"><strong>Position (y축)</strong>: 시퀀스 내 토큰의 위치 (0, 1, 2, ..., 49)</li></ul><ul id="4870c144-235b-462f-a160-32b2c457fc8e" class="bulleted-list"><li style="list-style-type:disc"><strong>Depth (x축)</strong>: 임베딩 벡터의 각 차원 위치 (0, 1, 2, ..., ~120)</li></ul><ul id="92968636-0f35-4638-9fa6-a87a96940eaa" class="bulleted-list"><li style="list-style-type:disc"><strong>색상</strong>: 각 위치-차원 조합의 인코딩 값 (파란색: 양수, 빨간색: 음수)</li></ul><p id="2024d778-1cdc-8087-b6ca-ce9b57976c87" class="">
</p><h3 id="df001785-e6f1-45da-89d1-65b4350ab82e" class="">구체적 예시</h3><p id="89c6c848-d95d-4183-8f4f-56f41930fbaa" class="">문장: &quot;나는 트랜스포머 모델을 공부한다&quot;</p><p id="aa80545d-77a0-44fc-a79b-d644be2145ae" class="">이 문장이 토큰화되면:</p><ul id="760fc5d1-0185-4d8a-8e7d-6368bb6e7f92" class="bulleted-list"><li style="list-style-type:disc">&quot;나는&quot; → 위치 0</li></ul><ul id="1c4c1339-11f5-4b34-834a-20eab56d69ec" class="bulleted-list"><li style="list-style-type:disc">&quot;트랜스포머&quot; → 위치 1</li></ul><ul id="20c9001c-85e3-4599-9e89-a4b85a2ce71b" class="bulleted-list"><li style="list-style-type:disc">&quot;모델을&quot; → 위치 2</li></ul><ul id="071ab247-dc48-4c19-a8a7-5ed0989506a8" class="bulleted-list"><li style="list-style-type:disc">&quot;공부한다&quot; → 위치 3</li></ul><p id="ce763b24-e655-4a07-86ba-086718369a0d" class="">각 토큰은 d_model 차원(예: 512)의 임베딩 벡터로 표현됩니다. 이 때:</p><ol type="1" id="2e2b3648-cc53-4f08-92c6-f9319e64a991" class="numbered-list" start="1"><li>&quot;나는&quot;(위치 0)의 임베딩 벡터에 PE(0, 0), PE(0, 1), ..., PE(0, 511) 값을 더함</li></ol><ol type="1" id="22044b9f-f617-416f-8426-38dab759ca83" class="numbered-list" start="2"><li>&quot;트랜스포머&quot;(위치 1)의 임베딩 벡터에 PE(1, 0), PE(1, 1), ..., PE(1, 511) 값을 더함</li></ol><ol type="1" id="b959f2cd-1e3b-4488-acc6-8028e1c9b549" class="numbered-list" start="3"><li>이하 동일하게 각 토큰 위치별로 적용</li></ol><p id="2024d778-1cdc-8097-ac0c-ce4a411d18d0" class="">
</p><ul id="a8f1a786-9c01-4aed-88bf-686065bc1959" class="bulleted-list"><li style="list-style-type:disc">왼쪽(낮은 depth): 빠르게 변화하는 패턴 - 고주파 성분</li></ul><ul id="b37325cc-bd51-4ffe-9534-b3b5b40129ad" class="bulleted-list"><li style="list-style-type:disc">오른쪽(높은 depth): 천천히 변화하는 패턴 - 저주파 성분</li></ul><p id="f3fc8514-73a0-492a-b783-ee1226720c3d" class="">이 다양한 주파수 패턴이 각 위치를 고유하게 인코딩하며, 모델이 상대적 위치 정보를 학습할 수 있게 합니다.</p><p id="2024d778-1cdc-807f-9fb7-e76233bfad3b" class="">
</p><h2 id="2a0e2961-5f67-4278-916f-969fe5b94cae" class="">왜 중요한가?</h2><ol type="1" id="507c4880-1195-4239-b769-76e6aaaaedf0" class="numbered-list" start="1"><li><strong>위치 구분</strong>: 서로 다른 위치는 서로 다른 포지셔널 인코딩 값을 가짐</li></ol><ol type="1" id="5377d5e4-985c-4192-aefd-c6892d2e4864" class="numbered-list" start="2"><li><strong>상대적 거리 학습</strong>: 비슷한 거리에 있는 토큰들은 비슷한 관계성을 가짐</li></ol><ol type="1" id="c8b9b617-cdac-47d0-beab-f540fc111cbf" class="numbered-list" start="3"><li><strong>Attention 메커니즘 보완</strong>: Self-attention은 위치 정보가 없으므로 이를 보완</li></ol><p id="69a69318-a0cf-493d-82db-1965271a95ff" class="">포지셔널 인코딩의 이러한 특성 덕분에 Transformer 모델은 순서 정보 없이도 시퀀스 데이터를 효과적으로 처리할 수 있습니다.</p><p id="2024d778-1cdc-805c-952c-d79fbf226313" class="">
</p><p id="2024d778-1cdc-8058-8760-cf70ef416328" class="">
</p><p id="2024d778-1cdc-80a1-bdf6-d6723dbc3050" class="">
</p><h3 id="9d7b9e7c-5e62-40db-adb4-20b8b4f4c5ef" class="">4. Self-Attention 계산 과정</h3><ol type="1" id="fb8fe685-0c78-4f9c-a13f-ee740b0fbfaa" class="numbered-list" start="1"><li><strong>Query, Key, Value 벡터 생성</strong><ul id="70d6c7bf-18aa-4f62-a545-55e55368c0fc" class="bulleted-list"><li style="list-style-type:disc">입력 벡터에 가중치 행렬을 곱하여 Q, K, V 벡터 생성</li></ul><ul id="8afb155a-7dfb-46f8-ae47-76b1682b4b72" class="bulleted-list"><li style="list-style-type:disc">Q = XW^Q, K = XW^K, V = XW^V</li></ul></li></ol><ol type="1" id="0e767e2f-879c-45c2-b06c-66be66646048" class="numbered-list" start="2"><li><strong>Attention 점수 계산</strong><ul id="103ff04e-df79-430b-b621-d31bf487e76b" class="bulleted-list"><li style="list-style-type:disc">Query와 Key의 내적으로 유사도 계산</li></ul><ul id="27eb7b16-59f7-479e-bb0f-884db8e759d4" class="bulleted-list"><li style="list-style-type:disc">스케일링 적용: QK^T / √d_k</li></ul></li></ol><ol type="1" id="a64796b1-dc0d-4740-8894-c05cca79afbc" class="numbered-list" start="3"><li><strong>Softmax 적용</strong><ul id="f040509b-b2b5-4981-8303-c76a79182902" class="bulleted-list"><li style="list-style-type:disc">점수를 확률 분포로 변환: softmax(QK^T / √d_k)</li></ul></li></ol><ol type="1" id="3c9258e3-bacd-41e7-907c-e25a4c9d46dd" class="numbered-list" start="4"><li><strong>Value 가중합</strong><ul id="5b5703ef-f44d-4faf-b18b-db2519c726ab" class="bulleted-list"><li style="list-style-type:disc">확률과 Value의 가중합 계산: softmax(QK^T / √d_k)V</li></ul></li></ol><p id="d209bd64-81ca-44cb-819b-f6c8ad225162" class="">최종 수식: Attention(Q, K, V) = softmax(QK^T / √d_k)V</p><figure id="2024d778-1cdc-80cd-9738-cfba311320b0" class="image"><a href="image%2078.png"><img style="width:709.96875px" src="image%2078.png"/></a></figure><p id="2024d778-1cdc-80c5-a37a-e4a97cb76f20" class="">
</p><p id="2024d778-1cdc-80e3-ab3e-eac32536d10c" class="">
</p><p id="2024d778-1cdc-801f-acad-e8a9372b8ccd" class="">
</p><figure id="2024d778-1cdc-802d-864f-c0509e90477a" class="image"><a href="image%2079.png"><img style="width:709.96875px" src="image%2079.png"/></a></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>