<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Pre-Trained Model(사전 학습 모델)</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.collection-content td {
	white-space: pre-wrap;
	word-break: break-word;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

blockquote.quote-large {
	font-size: 1.25em;
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.highlight-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.highlight-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.highlight-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.highlight-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.highlight-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.highlight-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.highlight-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.highlight-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.highlight-default_background {
	color: rgba(44, 44, 43, 1);
}
.highlight-gray_background {
	background: rgba(240, 239, 237, 1);
}
.highlight-brown_background {
	background: rgba(245, 237, 233, 1);
}
.highlight-orange_background {
	background: rgba(251, 235, 222, 1);
}
.highlight-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.highlight-teal_background {
	background: rgba(232, 241, 236, 1);
}
.highlight-blue_background {
	background: rgba(232, 242, 250, 1);
}
.highlight-purple_background {
	background: rgba(243, 235, 249, 1);
}
.highlight-pink_background {
	background: rgba(250, 233, 241, 1);
}
.highlight-red_background {
	background: rgba(252, 233, 231, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(142, 139, 134, 1);
	fill: rgba(142, 139, 134, 1);
}
.block-color-brown {
	color: rgba(182, 137, 101, 1);
	fill: rgba(182, 137, 101, 1);
}
.block-color-orange {
	color: rgba(213, 128, 59, 1);
	fill: rgba(213, 128, 59, 1);
}
.block-color-yellow {
	color: rgba(229, 178, 68, 1);
	fill: rgba(229, 178, 68, 1);
}
.block-color-teal {
	color: rgba(85, 167, 124, 1);
	fill: rgba(85, 167, 124, 1);
}
.block-color-blue {
	color: rgba(35, 131, 226, 1);
	fill: rgba(35, 131, 226, 1);
}
.block-color-purple {
	color: rgba(181, 119, 214, 1);
	fill: rgba(181, 119, 214, 1);
}
.block-color-pink {
	color: rgba(219, 105, 153, 1);
	fill: rgba(219, 105, 153, 1);
}
.block-color-red {
	color: rgba(229, 100, 88, 1);
	fill: rgba(229, 100, 88, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(240, 239, 237, 1);
}
.block-color-brown_background {
	background: rgba(245, 237, 233, 1);
}
.block-color-orange_background {
	background: rgba(251, 235, 222, 1);
}
.block-color-yellow_background {
	background: rgba(249, 243, 220, 1);
}
.block-color-teal_background {
	background: rgba(232, 241, 236, 1);
}
.block-color-blue_background {
	background: rgba(232, 242, 250, 1);
}
.block-color-purple_background {
	background: rgba(243, 235, 249, 1);
}
.block-color-pink_background {
	background: rgba(250, 233, 241, 1);
}
.block-color-red_background {
	background: rgba(252, 233, 231, 1);
}
.select-value-color-default { background-color: rgba(42, 28, 0, 0.07); }
.select-value-color-gray { background-color: rgba(28, 19, 1, 0.11); }
.select-value-color-brown { background-color: rgba(127, 51, 0, 0.156); }
.select-value-color-orange { background-color: rgba(196, 88, 0, 0.203); }
.select-value-color-yellow { background-color: rgba(209, 156, 0, 0.282); }
.select-value-color-green { background-color: rgba(0, 96, 38, 0.156); }
.select-value-color-blue { background-color: rgba(0, 99, 174, 0.172); }
.select-value-color-purple { background-color: rgba(92, 0, 163, 0.141); }
.select-value-color-pink { background-color: rgba(183, 0, 78, 0.152); }
.select-value-color-red { background-color: rgba(206, 24, 0, 0.164); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1f64d778-1cdc-809c-b87c-da41c476e24a" class="page sans"><header><h1 class="page-title">Pre-Trained Model(사전 학습 모델)</h1><p class="page-description"></p></header><div class="page-body"><p id="1f64d778-1cdc-8018-a291-f2ef797f7d77" class="">사전 학습된 모델(Pre-trained Model)을 사용하면 <strong>큰 데이터셋으로 이미 학습된 지식과 특징을 재사용</strong>할 수 있어서 매우 효율적입니다. 예를 들어, 일반적인 이미지 인식 작업에서는 <strong>이미지의 엣지, 모양, 색상 패턴</strong> 등을 미리 학습한 모델을 사용할 수 있기 때문에, 처음부터 모든 데이터를 학습하지 않아도 <strong>시간을 절약</strong>하고 <strong>성능을 높일 수 있습니다</strong>.</p><p id="1f64d778-1cdc-80b7-a673-fe62ae69a12b" class="">PyTorch는 여러 유명한 사전 학습된 모델을 제공하며, 이를 간단히 불러와 사용할 수 있습니다. 아래는 PyTorch에서 사전 학습된 모델을 사용하는 방법을 예제로 설명해드리겠습니다.</p><hr id="1f64d778-1cdc-80da-be95-f35927874c57"/><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-yellow_background">PyTorch에서 사전 학습된 모델 사용법</mark></summary><div class="indented"><p id="1f64d778-1cdc-8022-94b5-cad69c0c3d5c" class="">PyTorch의 <code>torchvision.models</code> 모듈에는 다양한 사전 학습된 모델들이 포함되어 있습니다. 예를 들어, <strong>ResNet, VGG, Inception</strong> 등 여러 모델을 사용할 수 있으며, <code>pretrained=True</code> 파라미터를 설정하여 사전 학습된 가중치를 불러올 수 있습니다.</p><h3 id="1f64d778-1cdc-8056-a716-fd99b7faab1b" class="">예제: 사전 학습된 ResNet 모델 사용하기</h3><ol type="1" id="1f64d778-1cdc-803c-b99f-cc8f5e29d996" class="numbered-list" start="1"><li><strong>사전 학습된 ResNet 모델 불러오기</strong><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-80b7-bcda-cab24ba4b823" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch
import torchvision.models as models

# ResNet-18 모델 불러오기, pretrained=True 설정으로 사전 학습된 가중치 사용
model = models.resnet18(pretrained=True)</code></pre><p id="1f64d778-1cdc-80d7-908a-efa6af1cb93b" class="">이 코드에서 <code>models.resnet18(pretrained=True)</code>를 통해 <strong>사전 학습된 ResNet-18 모델</strong>을 불러옵니다. PyTorch는 모델을 불러올 때 <code>pretrained=True</code> 옵션을 사용하면, ImageNet 데이터셋으로 학습된 가중치가 자동으로 로드됩니다.</p></li></ol><ol type="1" id="1f64d778-1cdc-80a4-8507-da31ea670550" class="numbered-list" start="2"><li><strong>모델의 일부 레이어 고정(freeze)</strong><p id="1f64d778-1cdc-80f8-8f9c-fd876c055f78" class="">만약 특징 추출을 위해 모델을 사용하려는 경우, <strong>모델의 초반부를 고정</strong>하여 학습되지 않도록 설정할 수 있습니다.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-804f-b775-cfbd96f61d77" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 모델의 모든 파라미터를 고정하여 업데이트되지 않도록 설정
for param in model.parameters():
    param.requires_grad = False</code></pre></li></ol><ol type="1" id="1f64d778-1cdc-801b-8ba4-dfab5858cb31" class="numbered-list" start="3"><li><strong>새로운 분류 레이어 추가</strong><br/>이제 기존 모델의 분류 레이어를 새로운 작업에 맞게 변경할 수 있습니다. 예를 들어, ResNet-18의 마지막 <code>fc</code> 레이어를 고양이와 개를 분류하는 이진 분류용 레이어로 교체할 수 있습니다.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-80ee-9fc1-c18d71069d2c" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 출력 클래스의 수를 2개로 설정하여 새로운 fc 레이어를 추가
model.fc = torch.nn.Linear(in_features=512, out_features=2)</code></pre><p id="1f64d778-1cdc-80e8-8784-e5b96b06145d" class="">여기서 <code>512</code>는 ResNet-18의 마지막 레이어가 출력하는 특징 벡터의 크기입니다. 이 새로운 <code>fc</code> 레이어는 고양이와 개를 분류하는 <strong>2개의 클래스</strong>에 맞춰져 있습니다.</p><ul id="1f64d778-1cdc-8047-be21-ec2bebdf42e9" class="toggle"><li><details open=""><summary><strong>사전 학습 모델의 마지막 층 개수 확인하는 방법</strong></summary><p id="1f64d778-1cdc-80f1-8f4b-d7d0ce79b617" class="">각 사전 학습된 모델의 마지막 레이어의 출력 크기(특징 벡터 크기)는 모델마다 다릅니다. <strong>모델 구조를 확인하는 방법</strong>을 몇 가지 알려드리겠습니다.</p><hr id="1f64d778-1cdc-800f-92ba-e06cd6ce5917"/><h3 id="1f64d778-1cdc-8017-b0c7-e26538f5455b" class="">1. PyTorch 모델의 구조 출력하여 확인하기</h3><p id="1f64d778-1cdc-80c3-bb50-fc2a59dbc7df" class="">모델의 구조를 직접 출력하여 확인할 수 있습니다. <code>print(model)</code>을 사용하면 모델의 각 레이어와 출력 크기를 볼 수 있습니다.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-80e9-bde3-ec746f037470" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torchvision.models as models

# ResNet-18 모델 불러오기
model = models.resnet18(pretrained=True)

# 모델 구조 출력
print(model)</code></pre><p id="1f64d778-1cdc-801f-a239-ccffdde543e7" class="">출력 예시:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-805a-a310-c9a0f27c54a3" class="code code-wrap"><code class="language-Plain Text" style="white-space:pre-wrap;word-break:break-all">ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  ...
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)</code></pre><p id="1f64d778-1cdc-805e-bf37-e66421bd644e" class="">이 출력에서 <code>fc</code> 레이어에 <code>in_features=512</code>가 표시되므로, ResNet-18의 마지막 특징 벡터 크기가 512임을 알 수 있습니다.</p><hr id="1f64d778-1cdc-8030-8837-ecbdb846c0a2"/><h3 id="1f64d778-1cdc-805e-abef-ded19e23e7fb" class="">2. PyTorch 공식 문서에서 확인하기</h3><p id="1f64d778-1cdc-8098-a1ea-e04b2ac2fd56" class="">PyTorch의 torchvision.models 페이지에서는 각 모델의 세부 구조를 확인할 수 있습니다. 예를 들어, ResNet-18, VGG-16 등 다양한 모델에 대한 설명과 구조를 볼 수 있으며, 마지막 레이어의 <code>in_features</code> 정보도 확인할 수 있습니다.</p><hr id="1f64d778-1cdc-8020-8d51-f7595d48c123"/><h3 id="1f64d778-1cdc-8064-ab44-fa93966e2c62" class="">3. 코드에서 프로그램적으로 출력 크기 확인하기</h3><p id="1f64d778-1cdc-806f-afa9-fbecfb59e7e6" class="">모델의 특정 부분을 통과한 후의 출력 크기를 <strong>코드로 직접 확인</strong>할 수도 있습니다. 이는 사전 학습된 모델을 일부 고정하고 새로운 작업에 맞게 적용할 때 유용합니다.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-80ce-a4f0-fca50fbbeb18" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch
import torchvision.models as models

# 임의의 입력 텐서 생성 (예: 224x224 크기의 이미지)
dummy_input = torch.randn(1, 3, 224, 224)

# ResNet-18 모델 불러오기
model = models.resnet18(pretrained=True)

# FC 레이어 이전까지의 특징 추출 부분만 가져오기
features = model.avgpool(model.layer4(model.layer3(model.layer2(model.layer1(model.conv1(dummy_input))))))
features = features.view(features.size(0), -1)  # 평탄화

# 출력 크기 확인
print(features.shape)  # torch.Size([1, 512])</code></pre><p id="1f64d778-1cdc-80b9-b29a-d016aef0921b" class="">위 코드에서 <code>features.shape</code>의 출력은 <code>[1, 512]</code>가 되므로, ResNet-18의 마지막 특징 벡터 크기가 512라는 것을 알 수 있습니다.</p><hr id="1f64d778-1cdc-8085-85a3-cc85acc50b42"/><p id="1f64d778-1cdc-808f-88af-c2457e52331b" class="">이처럼 모델 구조를 출력하거나 PyTorch 문서를 참고하고, 또는 실제 데이터로 테스트하여 마지막 특징 벡터 크기를 확인할 수 있습니다.</p><p id="1f64d778-1cdc-809b-b734-c306922dba2d" class="">
</p></details></li></ul><p id="1f64d778-1cdc-8027-a097-e5118dcac1ba" class="">
</p></li></ol><ol type="1" id="1f64d778-1cdc-809b-adb7-d688572995ed" class="numbered-list" start="4"><li><strong>모델 학습</strong><br/>새로운 데이터로 모델을 학습합니다. <code>fc</code> 레이어만 업데이트되도록 설정했기 때문에 학습이 빠르고, 기존에 학습된 이미지의 일반적인 특징을 그대로 사용할 수 있습니다.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-8093-bcb7-de1d6060dd69" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># 간단한 예제로 옵티마이저 설정 및 학습 루프 (예시)
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()

# 예시 학습 루프 (trainloader는 새로운 데이터 로더라고 가정)
for epoch in range(5):
    for inputs, labels in trainloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

</code></pre></li></ol><p id="1f64d778-1cdc-8025-a13e-d8079e9455ed" class="">이렇게 사전 학습된 모델을 사용하여 필요한 부분만 새롭게 학습하게 함으로써, 모델의 <strong>학습 시간과 리소스</strong>를 크게 절약할 수 있습니다. 또한, ImageNet과 같은 대규모 데이터셋의 일반적인 특징을 활용할 수 있기 때문에, 적은 데이터로도 좋은 성능을 기대할 수 있습니다.</p><h3 id="1f64d778-1cdc-8093-a96c-e89b2992f960" class="">요약</h3><ul id="1f64d778-1cdc-8015-8d6e-c092345ce498" class="bulleted-list"><li style="list-style-type:disc"><strong>사전 학습된 모델 사용의 장점</strong>: 시간 절약, 데이터 부족 문제 해결, 기존 특징을 활용한 높은 성능.</li></ul><ul id="1f64d778-1cdc-805b-af0a-d785f9ab269e" class="bulleted-list"><li style="list-style-type:disc"><strong>PyTorch에서의 사용 방법</strong>: <code>torchvision.models</code>에서 <code>pretrained=True</code>로 불러오고, 필요한 레이어만 조정하여 학습.</li></ul><p id="1f64d778-1cdc-80dd-b66a-ea8b8b56b9cd" class="">이 방법을 통해 새로운 데이터나 작업에서도 효율적으로 전이 학습을 수행할 수 있습니다.</p><p id="1f64d778-1cdc-8059-9742-f0c14bb12a17" class="">
</p><h3 id="1f64d778-1cdc-80c6-abd4-cfacaa0e2a61" class="">전체코드 흐름</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1f64d778-1cdc-80c8-ba3b-fd53a2c3cab2" class="code code-wrap"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder

# 데이터셋 준비 (ImageFolder를 통한 예시)
# 데이터 전처리
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # ResNet-18에 맞는 크기
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet의 ImageNet 정규화 기준
])

# 데이터 로더 생성
train_dataset = ImageFolder(root=&#x27;path/to/train_data&#x27;, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)


# 사전 학습된 ResNet-18 모델 불러오기
model = models.resnet18(pretrained=True)

# 기존 모델의 파라미터 고정 (초반부 레이어는 고정, 안해도 됨)
for param in model.parameters():
    param.requires_grad = False

# 모델의 마지막 레이어 교체 (새로운 클래스 수에 맞춰 조정)
num_classes = 2  # 분류할 클래스 수 (예: 2개의 클래스)
model.fc = nn.Linear(in_features=model.fc.in_features, out_features=num_classes)

# 사용할 손실 함수와 옵티마이저 정의
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)

# Fine-Tuning 학습 루프
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for inputs, labels in train_loader:
        # 데이터를 GPU로 전송 (옵션)
        # inputs, labels = inputs.to(device), labels.to(device)

        # 옵티마이저 초기화
        optimizer.zero_grad()

        # 순전파 및 손실 계산
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # 역전파 및 옵티마이저 스텝
        loss.backward()
        optimizer.step()

        # 손실 축적
        running_loss += loss.item()

    # Epoch마다 평균 손실 출력
    print(f&quot;Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}&quot;)

print(&quot;Fine-Tuning 완료&quot;)</code></pre></div></details><p id="1fa4d778-1cdc-802d-9350-f1db5ecfd9bb" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>