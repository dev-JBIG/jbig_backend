<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>데이터 전처리</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(115, 114, 110, 1);
	fill: rgba(115, 114, 110, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(205, 60, 58, 1);
	fill: rgba(205, 60, 58, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(115, 114, 110, 1);
	fill: rgba(115, 114, 110, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(205, 60, 58, 1);
	fill: rgba(205, 60, 58, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-default { background-color: rgba(84, 72, 49, 0.08); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1b44d778-1cdc-80c4-a995-e211c9ac80e7" class="page sans"><header><h1 class="page-title">데이터 전처리</h1><p class="page-description"></p></header><div class="page-body"><h1 id="1b44d778-1cdc-818d-a155-c3553c837e86" class="">결측치 처리</h1><ul id="1b44d778-1cdc-8105-a0b5-ee82d3557694" class="toggle"><li><details open=""><summary>결측치 처리</summary><p id="1b44d778-1cdc-8152-b8a0-d88ce909c6cc" class="">데이터 분석 과정에서 결측치(missing values)는 흔히 접하게 되는 중요한 이슈입니다. 결측치를 어떻게 처리하느냐에 따라 모델의 성능뿐만 아니라 해석 가능성에도 큰 영향을 미치므로, 데이터를 다룰 때 항상 신중하게 접근해야 합니다. 아래에서는 결측치의 정의, 결측치가 모델에 미치는 영향, 그리고 이를 해결하기 위한 대표적인 방법들을 순서대로 알아보겠습니다.</p><hr id="1b44d778-1cdc-812d-a964-fc299e82020d"/><h2 id="1b44d778-1cdc-81b7-88bb-d8fd6a8fc8ff" class="">1. 결측치란?</h2><ul id="1b44d778-1cdc-81b1-84a1-fef47d9a6e0d" class="bulleted-list"><li style="list-style-type:disc"><strong>결측치(missing value)</strong>란, 특정 변수(컬럼)에 원래 존재해야 할 관측값이 누락된 상태를 말합니다.</li></ul><ul id="1b44d778-1cdc-818b-8160-d3c20369939d" class="bulleted-list"><li style="list-style-type:disc">예를 들어, 설문조사에서 응답자가 나이를 기입하지 않았다면(빈 칸), 해당 레코드는 나이 변수에 대해 결측치가 됩니다.</li></ul><ul id="1b44d778-1cdc-8173-8264-e54f2e808e53" class="bulleted-list"><li style="list-style-type:disc">머신러닝, 통계분석에서 결측치는 알고리즘에 직접 입력할 수 없는 경우가 대부분이므로, 처리가 필수적입니다.</li></ul><hr id="1b44d778-1cdc-81d3-a507-e8444deff3b6"/><h2 id="1b44d778-1cdc-81de-bc9c-c7b08a8a7389" class="">2. 결측치가 모델에 미치는 영향</h2><ol type="1" id="1b44d778-1cdc-813e-9460-c2b90da032a1" class="numbered-list" start="1"><li><strong>데이터 손실</strong><ul id="1b44d778-1cdc-810f-912f-d1ed59575545" class="bulleted-list"><li style="list-style-type:disc">결측치가 많을수록 실제로 모델이 학습할 수 있는 유효한 데이터가 줄어듭니다.</li></ul><ul id="1b44d778-1cdc-81c6-9097-ecb5eba4d82a" class="bulleted-list"><li style="list-style-type:disc">만약 무작정 결측치가 있는 행(또는 열)을 제거해버리면 표본의 수가 현저히 줄어들 가능성이 있으므로, 이는 모델의 성능 저하와 통계적 파워(power)의 감소로 이어질 수 있습니다.</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81e2-83e3-d2adab4b4304" class="numbered-list" start="2"><li><strong>편향(Bias) 유발</strong><ul id="1b44d778-1cdc-8111-bb1d-e9f239f94f70" class="bulleted-list"><li style="list-style-type:disc">결측이 단순히 무작위로 발생한다면 상대적으로 덜하지만, 특정 패턴(예: 나이가 많을수록 질문에 응답을 안 했다)이 있을 경우에는 데이터의 분포가 왜곡(bias)될 수 있습니다.</li></ul><ul id="1b44d778-1cdc-8128-984d-de3acbc3a704" class="bulleted-list"><li style="list-style-type:disc">편향된 데이터로 학습된 모델은 일반화 성능이 떨어지고 왜곡된 예측 결과를 낳게 됩니다.</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81d3-9566-e8f8ebbba704" class="numbered-list" start="3"><li><strong>모델 훈련 및 예측에 직접적 장애</strong><ul id="1b44d778-1cdc-8122-a29b-d1649719bebb" class="bulleted-list"><li style="list-style-type:disc">일부 알고리즘(예: 회귀분석, 트리 기반 모델)은 결측치를 스스로 처리할 수 있거나(트리 기반 모델 중 LightGBM 등) 무시할 수 있는 기법이 있기는 하지만, 대부분의 머신러닝 알고리즘은 결측치를 입력값으로 허용하지 않습니다. 따라서 결측치 처리를 반드시 진행해야 합니다.</li></ul></li></ol><hr id="1b44d778-1cdc-8140-8ba6-df0f1a6d4196"/><h2 id="1b44d778-1cdc-8194-9827-dfee08a0f88d" class="">3. 결측치 해결 방법</h2><p id="1b44d778-1cdc-81d7-9fcc-ec27ad97704d" class="">결측치를 처리하는 대표적인 방법은 크게 ‘삭제(제거)’와 ‘대체(보간, Imputation)’로 나눌 수 있습니다. 어떤 방법을 쓸지 결정할 때에는 데이터의 특성과 결측 발생 메커니즘(MCAR, MAR, MNAR 등)을 고려해야 합니다.</p><h3 id="1b44d778-1cdc-81c4-8377-f056a8e26b81" class="">3.1 삭제(제거)를 통한 처리</h3><ol type="1" id="1b44d778-1cdc-81a9-ac6c-ee8bc550eb0c" class="numbered-list" start="1"><li><strong>행(레코드) 단위 삭제 (Listwise Deletion)</strong><ul id="1b44d778-1cdc-8186-9701-f35b9aacf50a" class="bulleted-list"><li style="list-style-type:disc">결측치가 있는 행 전체를 제거합니다.</li></ul><ul id="1b44d778-1cdc-8170-829f-c64e2eaf6c44" class="bulleted-list"><li style="list-style-type:disc">가장 간단한 방법이지만, 결측치가 많은 열에서 이 방식을 사용하면 전체 데이터 중 다수의 행이 삭제될 수 있어 데이터 손실이 큰 편입니다.</li></ul><ul id="1b44d778-1cdc-8182-a9c6-d97a83de1add" class="bulleted-list"><li style="list-style-type:disc">결측치가 매우 적고, 해당 데이터를 제거해도 통계적 파워나 모델 성능에 큰 영향이 없다고 판단될 때 사용합니다.</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81df-affa-d0e93da7eaeb" class="numbered-list" start="2"><li><strong>열(변수) 단위 삭제 (Column Deletion)</strong><ul id="1b44d778-1cdc-813d-adf5-e3c930b07625" class="bulleted-list"><li style="list-style-type:disc">결측치가 지나치게 많은 특정 컬럼을 통째로 제거합니다.</li></ul><ul id="1b44d778-1cdc-816c-8332-e46cfb857d52" class="bulleted-list"><li style="list-style-type:disc">해당 변수가 분석 및 모델링에서 결정적으로 중요한 변수가 아니라면 고려할 수 있지만, 핵심적인 예측 변수라면 피해야 합니다.</li></ul></li></ol><blockquote id="1b44d778-1cdc-8171-a3a3-dff99b61cac9" class="">장점<ul id="1b44d778-1cdc-81b8-a74a-d793f9bb1640" class="bulleted-list"><li style="list-style-type:disc">단순하고 빠르게 적용 가능</li></ul><ul id="1b44d778-1cdc-8105-9793-f23c9c4d9586" class="bulleted-list"><li style="list-style-type:disc">추가적인 가정이나 추정을 하지 않아 분석의 해석성을 유지하기 쉬움</li></ul><p id="1b44d778-1cdc-81ad-9241-f594125b49c8" class=""><strong>단점</strong></p><ul id="1b44d778-1cdc-8111-9b17-d97ba8e6e43c" class="bulleted-list"><li style="list-style-type:disc">값비싼 정보 손실 발생 가능</li></ul><ul id="1b44d778-1cdc-81fe-adc8-de8501f9f02c" class="bulleted-list"><li style="list-style-type:disc">결측 데이터에 패턴이 있을 경우 편향 발생 가능</li></ul></blockquote><hr id="1b44d778-1cdc-81d2-aebf-e34d659e8201"/><h3 id="1b44d778-1cdc-8184-9f59-cb4cdb12a7b4" class="">3.2 대체(보간, Imputation)를 통한 처리</h3><p id="1b44d778-1cdc-81b7-8e60-fc8b989be682" class="">결측치를 제거하지 않고, 적절한 방법으로 결측값을 추정하여 실제로 값을 채워넣는 방법입니다.</p><p id="1b44d778-1cdc-8127-843b-ec2e3c97aaa3" class="">가장 흔히 사용되는 방식부터 점차 복잡한 기법까지 살펴보겠습니다.</p><ol type="1" id="1b44d778-1cdc-8116-a572-cf8934f79706" class="numbered-list" start="1"><li><strong>단순 대체 (Simple Imputation)</strong><ul id="1b44d778-1cdc-81c7-914f-ed05f84f2c06" class="bulleted-list"><li style="list-style-type:disc"><strong>평균(mean) 또는 중앙값(median) 대체</strong><ul id="1b44d778-1cdc-8193-8867-de11a93e6401" class="bulleted-list"><li style="list-style-type:circle">연속형 변수(숫자형 데이터)에 주로 사용됩니다.</li></ul><ul id="1b44d778-1cdc-81de-b975-fbbaee21e373" class="bulleted-list"><li style="list-style-type:circle">단순하지만, 원 데이터의 분산을 과소추정(Variance Underestimation)할 수 있으므로 주의가 필요합니다.</li></ul></li></ul><ul id="1b44d778-1cdc-816e-bff9-f06424a54a2a" class="bulleted-list"><li style="list-style-type:disc"><strong>최빈값(mode) 대체</strong><ul id="1b44d778-1cdc-810c-b869-c177d186cb56" class="bulleted-list"><li style="list-style-type:circle">범주형 변수(카테고리형 데이터)에서 사용됩니다.</li></ul></li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81e3-8679-c11aeae29361" class="numbered-list" start="2"><li><strong>K-최근접 이웃(KNN) 대체</strong><ul id="1b44d778-1cdc-814f-9b38-e7947dd8b76c" class="bulleted-list"><li style="list-style-type:disc">결측값이 있는 샘플과 유사한(가까운) 다른 샘플들을 찾은 뒤, 그들의 값을 이용해 결측값을 추정하는 방식입니다.</li></ul><ul id="1b44d778-1cdc-8199-966e-c8322da93f90" class="bulleted-list"><li style="list-style-type:disc">관측값 간의 유사도를 측정하기 위해 거리를 계산하며, 이웃의 숫자(k)와 거리 측정 방법에 따라 결과가 달라질 수 있습니다.</li></ul><ul id="1b44d778-1cdc-8177-af8f-e6293a2ea7fb" class="bulleted-list"><li style="list-style-type:disc">데이터 차원이 높을수록(많은 변수를 다룰수록) 거리 계산에 주의해야 합니다(차원의 저주).</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81e3-b518-ecb1d0e26c07" class="numbered-list" start="3"><li><strong>회귀(Regression)를 활용한 대체</strong><ul id="1b44d778-1cdc-81a5-8160-cdb40c8d80d0" class="bulleted-list"><li style="list-style-type:disc">결측값이 있는 변수를 종속변수(타깃)로 설정하고, 다른 변수를 독립변수로 하여 회귀 모델(선형 회귀, 랜덤 포레스트 등)을 학습시키고 결측값을 예측합니다.</li></ul><ul id="1b44d778-1cdc-8179-aab8-c145ac050016" class="bulleted-list"><li style="list-style-type:disc">조금 더 정교한 접근으로, 해당 변수를 예측하는 모델을 만들어 채워넣는 방식입니다.</li></ul></li></ol><blockquote id="1b44d778-1cdc-81cd-902d-e999730f4a44" class="">장점<ul id="1b44d778-1cdc-81a2-bf7d-e7434b813550" class="bulleted-list"><li style="list-style-type:disc">값(정보) 손실이 적음</li></ul><ul id="1b44d778-1cdc-813e-9f1f-d8d1954be9d3" class="bulleted-list"><li style="list-style-type:disc">편향을 완화하고 모델 성능을 향상시킬 수 있음</li></ul><p id="1b44d778-1cdc-8146-adc7-efd2e877e61a" class=""><strong>단점</strong></p><ul id="1b44d778-1cdc-8142-a832-f5f9c2f0bea1" class="bulleted-list"><li style="list-style-type:disc">추가적인 가정과 모델링이 필요하기 때문에 복잡함 증가</li></ul><ul id="1b44d778-1cdc-817a-b8ce-c479d74ebd16" class="bulleted-list"><li style="list-style-type:disc">잘못된 가정이나 부정확한 모델을 사용할 경우 오히려 노이즈를 증가시킬 위험 존재</li></ul></blockquote><hr id="1b44d778-1cdc-8171-8539-de187a95de0d"/><h2 id="1b44d778-1cdc-818a-bef8-d22ba928e1f0" class="">4. 결측치 처리 시 고려 사항 및 요약</h2><ol type="1" id="1b44d778-1cdc-8159-9581-cf607f54228d" class="numbered-list" start="1"><li><strong>결측치 발생 메커니즘 파악</strong><ul id="1b44d778-1cdc-8138-93ee-c8c3476fc914" class="bulleted-list"><li style="list-style-type:disc">MCAR(무작위 결측): 결측치가 완전히 임의로 발생</li></ul><ul id="1b44d778-1cdc-812b-8475-c0db1a4c5ea4" class="bulleted-list"><li style="list-style-type:disc">MAR(조건부 무작위 결측): 관찰된 변수들에 의해 발생 확률이 달라질 수 있음</li></ul><ul id="1b44d778-1cdc-81b9-a7ed-f05bd13b379d" class="bulleted-list"><li style="list-style-type:disc">MNAR(비무작위 결측): 결측치 자체가 특정 패턴 또는 값의 영향을 받음</li></ul><ul id="1b44d778-1cdc-8161-830e-d1950dedd72a" class="bulleted-list"><li style="list-style-type:disc">결측 패턴을 이해하면, 어떤 방법을 사용하는 것이 합리적인지 판단에 도움을 줍니다.</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-8160-b427-da3589d30f53" class="numbered-list" start="2"><li><strong>데이터 탐색(EDA) 진행</strong><ul id="1b44d778-1cdc-8199-9ad6-f94520cc213e" class="bulleted-list"><li style="list-style-type:disc">결측치 비율, 결측치 분포, 다른 변수와의 상관관계 등을 확인하여 적합한 처리 방식을 결정합니다.</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81d8-880d-fdfae0c4cf75" class="numbered-list" start="3"><li><strong>성능 및 해석력 비교</strong><ul id="1b44d778-1cdc-8101-bd75-db0869c50a89" class="bulleted-list"><li style="list-style-type:disc">여러 결측치 처리 방법을 시도한 뒤, 모델 성능(정확도, RMSE 등)을 비교하고, 해석 관점(변수 중요도, 분포 왜곡 여부 등)도 함께 살펴봅니다.</li></ul></li></ol><ol type="1" id="1b44d778-1cdc-8167-89f3-fe9954059249" class="numbered-list" start="4"><li><strong>가능한 한 원본 데이터 수집 과정을 개선</strong><ul id="1b44d778-1cdc-81ea-b3fc-f08979a5c7ed" class="bulleted-list"><li style="list-style-type:disc">근본적으로는 결측치가 최소화되도록 데이터 수집 과정을 정비하는 것이 가장 좋습니다.</li></ul></li></ol><hr id="1b44d778-1cdc-8187-b053-d3399abcfea3"/><h2 id="1b44d778-1cdc-8138-887c-d50a6346ab96" class="">5. 결론</h2><ul id="1b44d778-1cdc-8191-ae05-f662f4a59b0b" class="bulleted-list"><li style="list-style-type:disc"><strong>결측치는 데이터에서 누락된 값</strong>으로, 유효 데이터 양을 감소시키고 편향을 유발하여 모델의 성능 및 해석 가능성에 부정적인 영향을 줄 수 있습니다.</li></ul><ul id="1b44d778-1cdc-8119-ade4-f9ce91c34924" class="bulleted-list"><li style="list-style-type:disc">결측치를 처리하는 방법은 크게 <strong>삭제</strong>와 <strong>대체</strong>로 나눌 수 있으며, 각각 장단점이 존재합니다.</li></ul><ul id="1b44d778-1cdc-81ce-8b9a-dc70dc3718f5" class="bulleted-list"><li style="list-style-type:disc">모델과 상황에 따라 가장 적합한 방식을 선택하고, 선택한 방법의 결과를 충분히 검증하는 과정이 필요합니다.</li></ul><p id="1b44d778-1cdc-8136-8da9-f49caa9e92a0" class="">결국 “<strong>데이터의 특성과 결측 발생 메커니즘을 충분히 이해하고, 그에 맞는 결측치 처리 기법을 신중히 적용</strong>”하는 것이 성공적인 데이터 분석과 모델링의 핵심이라 할 수 있습니다.</p></details></li></ul><h1 id="1b44d778-1cdc-8107-8b5b-fff5ba672003" class="">이상치 판단 및 처리</h1><ul id="1b44d778-1cdc-814f-a564-f741b3ee3dc2" class="toggle"><li><details open=""><summary>이상치(Outliers)란?</summary><p id="1b44d778-1cdc-81ed-8f38-e75318da06be" class=""><strong>이상치(Outlier)</strong>란 데이터셋에서 다른 관측값들과 현저히 다르거나 극단적인 값을 가지는 데이터 포인트를 의미합니다. 이는 데이터의 오류, 드문 이벤트, 또는 데이터 분포의 자연스러운 변동에 의해 발생할 수 있습니다. 예를 들어, 학생들의 시험 점수 데이터에서 대부분이 70~90점 사이에 분포하는데, 단 한 명이 0점이나 100점을 받았다면 이 값은 이상치로 간주될 수 있습니다.</p><figure id="1b44d778-1cdc-81dc-96d2-f65377ce1705" class="image"><a href="image.png"><img style="width:427px" src="image.png"/></a></figure><h3 id="1b44d778-1cdc-8101-8dbf-d0cf9414dad5" class="">1. <strong>이상치가 모델에 미치는 영향</strong></h3><p id="1b44d778-1cdc-81a5-b1d3-c8e8aa63de24" class="">이상치는 모델 학습 및 성능에 다양한 부정적 영향을 미칠 수 있습니다. 이로 인해 모델의 예측 능력과 일반화 성능이 저하될 수 있습니다.<div class="indented"><h3 id="1b44d778-1cdc-81ae-b9f1-dc9f043189c4" class=""><strong>1.1 모델 편향(Bias)</strong></h3><ul id="1b44d778-1cdc-8114-9d8f-c546d7752514" class="bulleted-list"><li style="list-style-type:disc"><strong>선형 회귀</strong>와 같은 회귀 모델에서는 이상치가 모델의 회귀선을 왜곡하여, 전체적인 경향을 잘못 추정하게 만들 수 있습니다. 이상치가 회귀선의 기울기와 절편에 큰 영향을 미치게 되어 데이터의 올바른 패턴을 학습하기 어렵게 합니다.</li></ul><ul id="1b44d778-1cdc-81c7-9256-eb27e4069706" class="bulleted-list"><li style="list-style-type:disc"><strong>분류 모델</strong>에서는 이상치가 클래스 경계를 잘못 형성하게 하여 분류 성능을 떨어뜨릴 수 있습니다.</li></ul><h3 id="1b44d778-1cdc-8180-ad2d-f246e2ff64ae" class=""><strong>1.2 모델의 분산 증가 (Variance)</strong></h3><ul id="1b44d778-1cdc-819e-a7c4-eaabb0e1e256" class="bulleted-list"><li style="list-style-type:disc">이상치는 모델의 불안정성을 높입니다. 예를 들어, 결정 트리나 k-최근접 이웃(KNN)과 같은 모델들은 이상치에 민감하게 반응하여 과적합(overfitting)되는 경향이 있습니다.</li></ul><ul id="1b44d778-1cdc-8115-9d67-f98b665626d5" class="bulleted-list"><li style="list-style-type:disc">과적합이 발생하면 모델은 훈련 데이터에서 높은 성능을 보이지만, 새로운 데이터에 대해 예측할 때 일반화 능력이 떨어져 성능이 저하됩니다.</li></ul><figure id="1b44d778-1cdc-8167-85cc-f544f132b7ab" class="image"><a href="image%201.png"><img style="width:596px" src="image%201.png"/></a></figure><h3 id="1b44d778-1cdc-812e-b3fe-fafc3dbb0429" class=""><strong>1.3 평가 지표 왜곡</strong></h3><ul id="1b44d778-1cdc-814c-8a68-d77fc5a13d7e" class="bulleted-list"><li style="list-style-type:disc">회귀 모델의 평가 지표인 <strong>평균 제곱 오차(MSE)</strong>는 오차를 제곱하여 합산하기 때문에 이상치의 영향을 크게 받습니다. 이는 평가 지표를 왜곡하여 모델의 실제 성능을 정확하게 평가하기 어렵게 합니다.</li></ul><ul id="1b44d778-1cdc-8176-8410-cad5cfd0ed12" class="bulleted-list"><li style="list-style-type:disc"><strong>평균 절대 오차(MAE)</strong>나 <strong>미디안 절대 오차</strong>와 같은 이상치에 덜 민감한 지표를 사용함으로써 이 문제를 완화할 수 있습니다.</li></ul><h3 id="1b44d778-1cdc-8188-8413-d82671f6c68d" class=""><strong>1.4 데이터 분포의 변형</strong></h3><ul id="1b44d778-1cdc-81d3-a9cd-c6a4cf325554" class="bulleted-list"><li style="list-style-type:disc">이상치는 데이터의 평균, 분산, 표준편차 등의 통계적 특성에 큰 영향을 미쳐 데이터 분포를 왜곡합니다. 이는 모델이 데이터의 올바른 분포와 패턴을 학습하는 데 어려움을 겪게 합니다.</li></ul><ul id="1b44d778-1cdc-8194-adbd-f80d2f9b87a2" class="bulleted-list"><li style="list-style-type:disc">특히, 정규 분포를 가정하는 모델(예: 선형 회귀, 로지스틱 회귀)에서는 데이터의 비정규성이 성능 저하로 이어질 수 있습니다.</li></ul></div></p><h3 id="1b44d778-1cdc-815d-be51-d6f6aec56894" class="">2. <strong>이상치 처리의 중요성</strong></h3><ul id="1b44d778-1cdc-8180-a029-ebc58e05e00c" class="bulleted-list"><li style="list-style-type:disc"><strong>모델 성능 향상</strong>: 이상치를 적절히 처리하면 데이터의 본질적인 패턴을 더 정확하게 학습할 수 있어 모델의 성능과 예측 정확도를 높일 수 있습니다.</li></ul><ul id="1b44d778-1cdc-81fc-b8aa-e05ed4567041" class="bulleted-list"><li style="list-style-type:disc"><strong>일반화 능력 개선</strong>: 과적합을 방지하여 새로운 데이터에 대한 일반화 능력을 향상시킬 수 있습니다.</li></ul><ul id="1b44d778-1cdc-81b7-b7e2-d4787bbd70be" class="bulleted-list"><li style="list-style-type:disc"><strong>해석 용이성</strong>: 모델의 학습 결과가 데이터의 실제 패턴을 반영하게 되어 모델 해석이 더 용이해집니다.</li></ul><h3 id="1b44d778-1cdc-8174-8359-e2c15ff9f46c" class="">3. <strong>이상치 처리 시 주의사항</strong></h3><ul id="1b44d778-1cdc-81cf-9569-eeb684932774" class="bulleted-list"><li style="list-style-type:disc"><strong>데이터의 맥락 파악</strong>: 모든 이상치가 데이터 오류를 나타내는 것은 아닙니다. 경우에 따라서는 중요한 정보를 포함하고 있는 의미 있는 값일 수 있습니다. 예를 들어, 질병 데이터에서 극단적인 수치가 특정한 질병의 징후를 나타낼 수 있습니다. 따라서 이상치를 제거하거나 수정하기 전에 데이터의 맥락을 파악해야 합니다.</li></ul><ul id="1b44d778-1cdc-81d8-915a-e1acd9c0813f" class="bulleted-list"><li style="list-style-type:disc"><strong>지나친 제거의 위험</strong>: 이상치를 무조건적으로 제거하면 데이터의 중요한 특성을 잃어버릴 수 있습니다. 특히 데이터의 양이 적은 경우, 이상치를 제거하면 데이터셋이 더 작아져 모델 학습이 어려워질 수 있습니다.</li></ul><ul id="1b44d778-1cdc-8160-b6c1-d09cb5d0c5c9" class="bulleted-list"><li style="list-style-type:disc"><strong>처리 방법 선택</strong>: 이상치를 확인하고 처리하는 방법은 데이터의 특성과 분석 목적에 따라 달라야 합니다. 예를 들어, 회귀 모델에서는 이상치에 덜 민감한 평가 지표(MAE, 미디안 절대 오차)나 로버스트 회귀를 사용하여 이상치의 영향을 완화할 수 있습니다.</li></ul></details></li></ul><ul id="1b64d778-1cdc-80bc-a9cd-c628f448caeb" class="toggle"><li><details open=""><summary>M<mark class="highlight-default"><mark class="highlight-default_background">ethods</mark></mark></summary><ul id="407b1327-c1e1-440f-8dde-49dd5977f7ab" class="toggle"><li><details open=""><summary><mark class="highlight-default"><mark class="highlight-default_background">시각적 방법</mark></mark></summary><ul id="6b3917bd-2ff6-4860-93da-6d7e4d251364" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><mark class="highlight-default_background"><strong>박스플롯(Boxplot)</strong></mark></mark><mark class="highlight-default"><mark class="highlight-default_background">: 박스플롯은 데이터의 중앙값, 사분위수, 그리고 이상치를 시각적으로 보여주는 도구입니다. 박스플롯의 whiskers을 벗어나는 데이터 포인트를 이상치로 간주합니다.</mark></mark></li></ul><ul id="4e78ad35-514c-44d2-b0c7-6c2a723963fd" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><mark class="highlight-default_background"><strong>산점도(Scatter plot)</strong></mark></mark><mark class="highlight-default"><mark class="highlight-default_background">: 두 변수 간의 관계를 시각화할 때 사용하며, 다른 데이터들과 현저히 다른 포인트들을 찾아낼 수 있습니다.</mark></mark></li></ul><ul id="ca20a3e3-afc7-4d4e-af93-86415f3df6b1" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default"><mark class="highlight-default_background"><strong>히스토그램(Histogram)</strong></mark></mark><mark class="highlight-default"><mark class="highlight-default_background">: 데이터의 분포를 보여줌으로써, 분포의 꼬리 부분이나 중심에서 멀리 떨어진 값을 이상치로 확인할 수 있습니다.</mark></mark></li></ul><figure id="3bd29986-b04e-4f18-b32b-f571839913ae" class="image"><a href="image%202.png"><img style="width:489px" src="image%202.png"/></a></figure></details></li></ul><ul id="8f6fa015-cc05-4e4d-9bb4-f04949716837" class="toggle"><li><details open=""><summary><mark class="highlight-default"><mark class="highlight-default_background">통계적 방법</mark></mark></summary><ul id="ab2231b2-4824-4727-a141-db971a5dbf69" class="toggle"><li><details open=""><summary><mark class="highlight-default"><mark class="highlight-default_background"><strong>Z-score</strong></mark></mark></summary><ul id="ab9737a8-dfd6-4ee4-b7bc-f23a6383d065" class="bulleted-list"><li style="list-style-type:disc">Z-socre는 데이터의 각 값이 평균으로부터 몇 표준편차만큼 떨어져 있는지를 측정합니다.</li></ul><ul id="418b72a9-b3ff-4c67-a5f3-ed394fe47d99" class="bulleted-list"><li style="list-style-type:disc">Z-score가 절대값으로 3보다 크면(또는 특정 상황에 따라 2보다 크면) 이상치로 간주합니다.</li></ul><ul id="666950e2-dba1-4a7b-8db0-1898135ef47c" class="bulleted-list"><li style="list-style-type:disc">Z-score 공식<figure id="e65e1fc0-ae0d-4d4b-b9cb-f2dc9fc807e6" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Z</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>X</mi><mo>−</mo><mi>μ</mi><mo stretchy="false">)</mo></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">Z = \frac{(X - \mu)}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.113em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">μ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="dc300527-a186-4f00-a4b6-9dc373eedeff" class="">
</p><ul id="b8b31d78-d748-44a4-af22-78181f3bc2fb" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span><span>﻿</span></span> : 관측값</li></ul><ul id="46b5ee9a-3bb8-4b7e-a786-b379940058ad" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">μ</span></span></span></span></span><span>﻿</span></span> : 데이터의 평균</li></ul><ul id="14080ef5-bfe1-4658-9d3e-89a1e1ab4771" class="bulleted-list"><li style="list-style-type:circle"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span><span>﻿</span></span> : 표준편차</li></ul></li></ul><ul id="7de12af9-595b-4a9c-9ffa-7e304656b82a" class="bulleted-list"><li style="list-style-type:disc">위 식을 통해 계산된 Z-Score의 절댓값이 클 수록 표준정규분포 그래프상 평균에서 멀리 위치하고, 이는 곧 일반적이지 않은(이상치일 확률이 높은) 데이터임을 의미한다.<p id="dd995549-ec5d-4f61-8347-625ff7ea8263" class="">통상 Z-Score가 ±2 또는 ±3 이상인 경우를 이상치로 간주한다.</p><figure id="4ac44980-28a3-43d3-a4ec-15d86f93b408" class="image" style="text-align:center"><a href="Untitled.png"><img style="width:342px" src="Untitled.png"/></a></figure><ul id="15161be0-6581-4ca0-8f25-f92862205160" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c36673e7-ca31-47ca-87ef-0edcdae00abd" class="code"><code class="language-Python">import pandas as pd
import numpy as np

# 예시 데이터 생성
np.random.seed(42)
data = pd.DataFrame({
    &#x27;feature1&#x27;: np.random.normal(loc=50, scale=5, size=100),
    &#x27;feature2&#x27;: np.random.normal(loc=30, scale=3, size=100)
})

# Z-score 계산
from scipy.stats import zscore

# 각 컬럼에 대한 Z-score 계산
z_scores = np.abs(zscore(data))

# Z-score가 3 이상인 데이터를 이상치로 간주
threshold = 3
data_zscore_filtered = data[(z_scores &lt; threshold).all(axis=1)]

print(&quot;Z-score 방법으로 이상치 제거 후 데이터:&quot;)
print(data_zscore_filtered)
</code></pre></details></li></ul><p id="3a90a3a4-6f54-4da4-bdde-3e232d7c7426" class="">
</p></li></ul></details></li></ul><ul id="be3027f5-ec35-405b-84c4-40f76642c8d9" class="toggle"><li><details open=""><summary><mark class="highlight-default"><mark class="highlight-default_background"><strong>IQR (Interquartile Range)</strong></mark></mark></summary><p id="ad0cf17b-7f85-46f2-ba8e-aea37f754cca" class=""><strong>IQR (Interquartile Range)</strong>이란 데이터의 사분위수를 활용해 이상치를 확인하는 방법입니다.</p><ul id="42d9ff1e-5ef6-4528-9dcc-b3beaaabb334" class="bulleted-list"><li style="list-style-type:disc"><strong>IQR</strong> = Q3 - Q1 (3사분위수 - 1사분위수)</li></ul><ul id="3d1b06ad-0d2a-49ac-82a5-fca7f686dc8f" class="bulleted-list"><li style="list-style-type:disc"><strong>이상치(Outliers) 기준</strong>: Q1 - 1.5 * IQR 보다 작거나,  Q3 + 1.5 * IQR 보다 큰 값.</li></ul><figure id="aae6ec27-138e-4d64-a367-b53917903592" class="image"><a href="image%203.png"><img style="width:592px" src="image%203.png"/></a></figure><ul id="94ff7d2e-1253-4def-89a6-80c7106fb3f3" class="bulleted-list"><li style="list-style-type:disc">최솟값(Minimum) : 제 1사분위(Q1)에서 1.5 IQR을 뺀 위치이다.</li></ul><ul id="29fcc708-0467-4434-94fd-ed64c9be2ff3" class="bulleted-list"><li style="list-style-type:disc">제 1사분위(Q1) : 25%의 위치를 의미한다.</li></ul><ul id="41b9879c-c330-48d0-89fb-055a2ae942dc" class="bulleted-list"><li style="list-style-type:disc">제 2사분위(Q2) : 50%의 위치로 중앙값(median)을 의미한다.</li></ul><ul id="c9a4e02b-2e6c-4516-8c52-e836294e5c34" class="bulleted-list"><li style="list-style-type:disc">제 3사분위(Q3) : 75%의 위치를 의미한다.</li></ul><ul id="c6ecfa6b-0da0-4880-9ba8-057c96296747" class="bulleted-list"><li style="list-style-type:disc">최댓값(Maximum) : 제 3사분위에서 1.5 IQR을 더한 위치이다.</li></ul><ul id="f82661dc-23a7-4156-a41b-3d8b370c81fe" class="bulleted-list"><li style="list-style-type:disc">IQR : (제 3사분위 값) - (제 1사분위 값)</li></ul><ul id="44aa2401-413c-4b78-8720-cde34dde54ac" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="dcbbb528-db51-47f0-a153-78c506d91b44" class="code"><code class="language-Python">import pandas as pd
import numpy as np

# 예시 데이터 생성
np.random.seed(42)
data = pd.DataFrame({
    &#x27;feature1&#x27;: np.random.normal(loc=50, scale=5, size=100),
    &#x27;feature2&#x27;: np.random.normal(loc=30, scale=3, size=100)
})

# IQR을 활용한 이상치 탐지 및 제거
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1

# IQR 범위 밖의 데이터를 이상치로 간주
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
data_iqr_filtered = data[~((data &lt; lower_bound) | (data &gt; upper_bound)).any(axis=1)]

print(&quot;IQR 방법으로 이상치 제거 후 데이터:&quot;)
print(data_iqr_filtered)
</code></pre></details></li></ul><p id="bf028446-de10-4cf6-ba17-58839a7cdae7" class="">
</p></details></li></ul></details></li></ul></details></li></ul><h1 id="1b44d778-1cdc-8133-95cc-faf6898f4cf6" class="">스케일링</h1><ul id="1b44d778-1cdc-81ce-9c06-f5e9c5f2afbd" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">개념</mark></summary><ul id="1b44d778-1cdc-8196-bf01-faab4c7ab240" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default_background">데이터 스케일링이란 데이터 전처리 과정 중의 하나입니다.</mark></li></ul><ul id="1b44d778-1cdc-8103-b307-ee03660148c4" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default_background">피처(feature)들마다 데이터값의 범위가 다 제각각이기 때문에 범위 차이가 클 경우 데이터를 갖고 모델을 학습할 때 0으로 수렴하거나 무한으로 발산할 수 있습니다.</mark></li></ul><ul id="1b44d778-1cdc-81ec-aec5-e10c7c884965" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default_background">따라서 데이터 스케일링을 통해 </mark><mark class="highlight-default_background"><strong>모든 피처들의 데이터 분포나 범위를 동일하게 조정</strong></mark><mark class="highlight-default_background">해줄 수 있습니다.</mark></li></ul><ul id="1b44d778-1cdc-8137-9ad9-d490a168f613" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default_background">트리 기반 알고리즘 같은 특수한 경우를 제외한 대다수의 머신러닝 알고리즘은 결과가 input features의 스케일에 큰 영향을 받음.</mark></li></ul><ul id="1b44d778-1cdc-81f8-83ed-ff017a022781" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-default_background">따라서 데이터의 스케일을 동일하게 맞춰주는 표준화 작업이 필요함.</mark></li></ul></details></li></ul><ul id="1b44d778-1cdc-8189-8115-d6f6b49f4b15" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">종류</mark></summary><ul id="1b44d778-1cdc-81f9-af3a-d0e03a3db980" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">MinMaxScaler</mark></summary><p id="1b44d778-1cdc-81bf-8c35-d69fe269fc48" class="">MinMaxScaler는 모든 피처들이 <strong>0과 1사이</strong>의 데이터값을 갖도록 만들어줍니다.</p><p id="1b44d778-1cdc-81f9-8f83-c0e445309200" class="">즉, 피처별로 최솟값은 0이 되고, 최댓값은 1이 되는 것이죠.</p><p id="1b44d778-1cdc-8111-8530-d61392073c25" class="">데이터가 2차원인 겅우, 모든 데이터는 x, y 축의 0과 1 사이에 존재하게 됩니다.</p><p id="1b44d778-1cdc-8126-a626-c0197a97226b" class="">이 방법 또한 이상치가 존재한다면, 이상치가 극값이 되어 데이터가 아주 좁은 범위에 분포하게 되기 때문에 스케일링 방법으로 적절하지 않습니다.</p><figure id="1b44d778-1cdc-813c-9817-e1121c98eb90" class="image"><a href="Untitled%201.png"><img style="width:191.00001525878906px" src="Untitled%201.png"/></a></figure><ul id="1b44d778-1cdc-814e-b6db-fc7976090575" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-817f-ad43-efe8cf4be054" class="code"><code class="language-Python">from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
mms.fit(X_train)
X_train_scaled = mms.transform(X_train)
X_test_scaled = mms.transform(X_test)
dtc.fit(X_train_scaled, y_train)
print(&#x27;모델의 정확도 :&#x27;, round(dtc.score(X_test_scaled, y_test), 4))</code></pre></details></li></ul></details></li></ul><ul id="1b44d778-1cdc-813c-b7ec-e88b17985a56" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">MaxAbsScaler</mark></summary><p id="1b44d778-1cdc-81cd-b789-f0a10f4a646c" class="">MaxAbsScaler는 MinMaxScaler와 비슷한데요!</p><p id="1b44d778-1cdc-8180-b7cc-c1ddc9f0d40f" class="">MaxAbs Scaling은 데이터의 최대 절댓값이 1, 0이 0으로 스케일링됩니다.</p><p id="1b44d778-1cdc-810e-96c2-cf4038160394" class="">즉, 절댓값이 0에서 1 사이로 매핑되므로 결과적으로 -1에서 1 사이로 스케일링됩니다.</p><p id="1b44d778-1cdc-811e-b0d0-ce86a9d5aec9" class="">마찬가지로, 이상치의 영향을 크게 받기 때문에 이상치가 존재할 경우 이 방법은 적절하지 않습니다.</p><figure id="1b44d778-1cdc-818c-91ab-f1c841e66736" class="image"><a href="image%204.png"><img style="width:399px" src="image%204.png"/></a></figure><ul id="1b44d778-1cdc-81b1-ae88-e5fc431074a6" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-81ce-bb03-d1b7b3f28af7" class="code"><code class="language-Python">from sklearn.preprocessing import MaxAbsScaler

mas = MaxAbsScaler()
mas.fit(X_train)
X_train_scaled = mas.transform(X_train)
X_test_scaled = mas.transform(X_test)
dtc.fit(X_train_scaled, y_train)
print(&#x27;모델의 정확도 :&#x27;, round(dtc.score(X_test_scaled, y_test), 4))</code></pre></details></li></ul></details></li></ul><ul id="1b44d778-1cdc-816e-9a4a-dd00c9f4eeff" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">StandardScaler</mark></summary><p id="1b44d778-1cdc-816d-906a-ca0e5d4f5d16" class=""><strong>표준화(Standardization)</strong>는 <strong>변수 각각의 평균을 0, 분산을 1로 만들어주는 스케일링 기법</strong>입니다. 표준화가 적용된 변수는 가우시안 정규분포를 가진 값으로 변환됩니다.</p><p id="1b44d778-1cdc-810b-a969-d3e01bb73812" class="">아래 수식과 같이, 변수 x의 원래 값에서 x의 평균을 뺀 값을 x의 표준편차로 나눈 값으로 계산할 수 있습니다.</p><figure id="1b44d778-1cdc-8128-b5f1-ec95aa92dd97" class="image"><a href="https://velog.velcdn.com/images/jiazzang/post/98d79c42-206a-4f1f-aab2-19ead7a3b49e/image.png"><img src="https://velog.velcdn.com/images/jiazzang/post/98d79c42-206a-4f1f-aab2-19ead7a3b49e/image.png"/></a></figure><p id="1b44d778-1cdc-8159-aa0a-dac3a41f6017" class="">이 방법의 경우에도 데이터 내에 이상치가 있다면 데이터의 평균과 분산에 크게 영향을 주기 때문에 스케일링 방법으로 적절하지 않습니다.</p><ul id="1b44d778-1cdc-8150-b943-fd4cbbb2588e" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-81df-9051-c0109a9416ef" class="code"><code class="language-Python">from sklearn.preprocessing import StandardScaler

std = StandardScaler()
std.fit(X_train)
X_train_scaled = std.transform(X_train)
X_test_scaled = std.transform(X_test)
dtc.fit(X_train_scaled, y_train)
print(&#x27;모델의 정확도 :&#x27;, round(dtc.score(X_test_scaled, y_test), 4))</code></pre></details></li></ul></details></li></ul><ul id="1b44d778-1cdc-811b-81e5-d3a4b8afc967" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">RobustScaler</mark></summary><p id="1b44d778-1cdc-8132-b04b-ee35ae1d4767" class="">RobustScaler는 StandardScaler와 비슷합니다.</p><p id="1b44d778-1cdc-8104-80bc-e3650cfbf27f" class="">다만, StandardScaler는 평균과 분산을 사용했지만 RobustScaler는 <strong>중간값</strong>(median)과 <strong>사분위값</strong>(quartile)을 사용합니다.</p><p id="1b44d778-1cdc-8118-a498-ff0e3f28ce55" class="">데이터의 중앙값 = 0, <a href="https://www.notion.so/553bdd95ecf74fb3ad0fb38829ba0747?pvs=21">IQR </a>= 1이 되도록 스케일링 합니다.</p><figure id="1b44d778-1cdc-810f-b64f-fc7249b850cd" class="image"><a href="image%205.png"><img style="width:399px" src="image%205.png"/></a></figure><p id="1b44d778-1cdc-8131-b2a4-f28f482531d9" class="">따라서, 이상치의 영향을 최소화할 수 있겠습니다.</p><p id="1b44d778-1cdc-8166-b302-fb8bf0376187" class="">StandardScaler와 비교하였을 때, RobustScaler를 활용하면 표준화 후 데이터가 더 넓게 분포해있습니다.</p><ul id="1b44d778-1cdc-8137-b900-f21f695f9727" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-81a5-bc49-feabe3515c46" class="code"><code class="language-Python">from sklearn.preprocessing import RobustScaler

rbs = RobustScaler()
X_train_scaled = rbs.fit_transform(X_train)
X_test_scaled = rbs.transform(X_test)
dtc.fit(X_train_scaled, y_train)
print(&#x27;모델의 정확도 :&#x27;, round(dtc.score(X_test_scaled, y_test), 4))</code></pre></details></li></ul></details></li></ul></details></li></ul><h1 id="1b44d778-1cdc-8127-b3f8-c3da08ff55d4" class="">데이터 불균형</h1><ul id="1b44d778-1cdc-81a6-80f5-f10832f13fae" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">개념</mark></summary><p id="1b44d778-1cdc-81f3-be8b-c8932bc939ec" class=""><strong>불균형 데이터란 정상 범주의 관측치 수와 이상 범주의 관측치 수가 현저히 차이나는 데이터</strong>를 말합니다.</p><p id="1b44d778-1cdc-81cf-86c0-e1864d52e3f4" class="">예를 들면, 암 발생 환자가 암에 걸리지 않은 사람보다 현저히 적고, 신용카드 사기 거래인 경우가 정상 거래인 경우보다 현저히 적습니다.</p><p id="1b44d778-1cdc-8133-99aa-eab1f41bf570" class="">이런 데이터들을 불균형 데이터라 볼 수 있습니다.</p><p id="1b44d778-1cdc-8145-85e4-c9d7bd4f5493" class="">
</p><p id="1b44d778-1cdc-81e2-9f16-feb8d2f86143" class="">정상을 정확히 분류하는 것과 이상을 정확히 분류하는 것 중 <strong>일반적으로 이상을 정확히 분류하는 것이 더 중요</strong>합니다.</p><p id="1b44d778-1cdc-81e1-bedb-ddc94ef5e366" class="">왜냐하면 <strong>보통 이상 데이터가 target값이 되는 경우가 많기 때문</strong>입니다.</p><figure id="1b44d778-1cdc-8182-a06f-fb52b041d2b0" class="image"><a href="https://blog.kakaocdn.net/dn/bx1Arc/btraBTQGurj/CQBfZLu102rNThkG7R1yX1/img.png"><img style="width:387px" src="https://blog.kakaocdn.net/dn/bx1Arc/btraBTQGurj/CQBfZLu102rNThkG7R1yX1/img.png"/></a></figure><p id="1b44d778-1cdc-8196-968b-f3796bfd6e30" class="">그림을 봤을 때 파란색은 정상 관측치이고 빨간색은 이상 관측치, 회색은 실제 이상 데이터의 분포를 나타낸 것입니다.</p><p id="1b44d778-1cdc-81f0-9923-d210d0587f3c" class="">즉, 회색 원은 아직 관측되지 않은 모르는 데이터입니다.</p><p id="1b44d778-1cdc-8133-9f6d-c3c236aab5bd" class="">하지만 경계선 왼쪽의 회색 원들은 실제로는 이상 데이터이기 때문에 정상 데이터로 오분류됩니다.</p><p id="1b44d778-1cdc-8152-a161-f48b40281bc2" class="">파란색과 빨간색의 데이터만 알고 있는 상태에서 학습을 시킬 경우 분류 경계선은 위의 그림과 같이 그어지게 됩니다.</p><p id="1b44d778-1cdc-8173-a5ff-e7820f61f6bf" class="">경계선은 파란색 원과 회색 원 사이에 그어져야 이상적인 경계선이라 할 수 있습니다.</p><p id="1b44d778-1cdc-8130-bf0d-d077bc78a281" class="">즉, <strong>불균형한 데이터 세트는 이상 데이터를 정확히 찾아내지 못할 수 있다는 문제점이 존재</strong>합니다.</p></details></li></ul><ul id="1b44d778-1cdc-8110-b258-f7ca56b01c0e" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">샘플링</mark></summary><figure id="1b44d778-1cdc-812c-9e7f-ef94d06dc9f6" class="image"><a href="image%206.png"><img style="width:257px" src="image%206.png"/></a></figure><ol type="1" id="1b44d778-1cdc-813d-a2ad-c0fa207f1f1b" class="numbered-list" start="1"><li>언더 샘플링<p id="1b44d778-1cdc-8125-a466-d78979fef8b2" class=""><mark class="highlight-default_background"><strong>언더 샘플링이란 다수 범주의 데이터를 소수 범주의 데이터 수에 맞게 줄이는 샘플링 방식</strong></mark><mark class="highlight-default_background">을 말합니다.</mark></p><h3 id="1b44d778-1cdc-81cc-b4a2-c078286611e1" class=""><mark class="highlight-default_background"><strong>Under-sampling의 장단점</strong></mark></h3><p id="1b44d778-1cdc-8199-88d7-dba3090da6bf" class=""><mark class="highlight-default_background"><strong>장점</strong></mark><mark class="highlight-default_background"> : 다수 Class 관측치 제거로 인한 계산 시간 감소, Class overlab 감소</mark></p><p id="1b44d778-1cdc-81fd-8010-e2cfb2a71727" class=""><mark class="highlight-default_background"><strong>단점</strong></mark><mark class="highlight-default_background"> : 데이터 제거로 인한 정보 손실 발생 (제거한 것이 분류에 큰 영향을 끼치는 데이터일 수 있음 !)</mark></p><ul id="1b44d778-1cdc-81ae-b3ec-d454403dd216" class="toggle"><li><details open=""><summary><mark class="highlight-default_background"><strong>Random Sampling</strong></mark></summary><p id="1b44d778-1cdc-815d-966c-dd55d811d13c" class=""><strong>Random Sampling이란 말 그대로 다수 범주에서 무작위로 샘플링을 하는 것</strong>입니다.</p><figure id="1b44d778-1cdc-8104-a36c-cd9922336390" class="image"><a href="https://blog.kakaocdn.net/dn/brBgjg/btrazujrZg6/RbK9sY5Ei4s7YaaqvFPUB0/img.png"><img style="width:700px" src="https://blog.kakaocdn.net/dn/brBgjg/btrazujrZg6/RbK9sY5Ei4s7YaaqvFPUB0/img.png"/></a></figure><p id="1b44d778-1cdc-8122-91f0-e752457a08f3" class="">3번의 Random Sampling을 한 결과인데 무작위로 샘플링을 하기 때문에 할 때마다 다른 결과를 얻는다는 단점이 존재합니다.</p><p id="1b44d778-1cdc-8194-902f-fd4fd877f3fa" class="">이를 보완하기 위해 나온 샘플링 방법이 Tomek Links 방법입니다.</p><ul id="1b44d778-1cdc-81a9-be3b-d78604a5bd3e" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-819e-94ed-ea143be2d303" class="code"><code class="language-Python">from collections import Counter
from sklearn.datasets import make_classification
from imblearn.under_sampling import RandomUnderSampler

# 샘플 데이터 생성 (불균형 데이터셋)
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)
print(f&#x27;Original dataset shape: {Counter(y)}&#x27;)

# 랜덤 언더샘플링 적용
rus = RandomUnderSampler(random_state=42)
X_res, y_res = rus.fit_resample(X, y)
print(f&#x27;Resampled dataset shape: {Counter(y_res)}&#x27;)
</code></pre></details></li></ul></details></li></ul><ul id="1b44d778-1cdc-8191-b4c6-f56876dc67aa" class="toggle"><li><details open=""><summary><mark class="highlight-default_background"><strong>Tomek Links</strong></mark></summary><p id="1b44d778-1cdc-8127-a3bf-ce44c4d961ff" class=""><strong>Tomek Links란 두 범주 사이를 탐지하고 정리를 통해 부정확한 분류경계선을  방지하는 방법</strong>입니다.</p><p id="1b44d778-1cdc-81c1-bcb0-fe28a6c6f050" class="">적용 방식은 먼저 Tomek Links를 형성한 후 다수 Class에 속한 관측치를 제거하는 방법입니다. Tomek Links를 형성하는 방법은 두 개의 다른 Class를 선으로 이었을 때, 두 Class 중간에 다른 관측치가 없는 경우 Link를 형성하게 됩니다. 아래의 그림처럼 두 개의 다른 Class가 Tomek Links 된 것을 볼 수 있습니다.</p><figure id="1b44d778-1cdc-81d5-80fa-e0af0ef63548" class="image"><a href="https://blog.kakaocdn.net/dn/cEhss6/btq74bnyN6i/osxT29fzZMMOQPRRrLzVf0/img.png"><img style="width:399px" src="https://blog.kakaocdn.net/dn/cEhss6/btq74bnyN6i/osxT29fzZMMOQPRRrLzVf0/img.png"/></a></figure><ul id="1b44d778-1cdc-81c4-a799-f0db5b444adc" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-8158-a300-fe3299e64b8d" class="code"><code class="language-Python">from collections import Counter
from sklearn.datasets import make_classification
from imblearn.under_sampling import TomekLinks

# 샘플 데이터 생성 (불균형 데이터셋)
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)
print(f&#x27;Original dataset shape: {Counter(y)}&#x27;)

# Tomek Links 언더샘플링 적용
tomek = TomekLinks()
X_res, y_res = tomek.fit_resample(X, y)
print(f&#x27;Resampled dataset shape: {Counter(y_res)}&#x27;)
</code></pre></details></li></ul></details></li></ul></li></ol><ol type="1" id="1b44d778-1cdc-81bd-9e07-c0839bbf5727" class="numbered-list" start="2"><li>오버 샘플링<p id="1b44d778-1cdc-81df-988a-fab9513b1cce" class=""><mark class="highlight-default_background"><strong>오버 샘플링이란 소수 범주의 데이터를 다수 범주의 데이터 수에 맞게 늘리는 샘플링 방식</strong></mark><mark class="highlight-default_background">을 말합니다.</mark></p><h3 id="1b44d778-1cdc-811f-a818-d2fbe39f952f" class=""><mark class="highlight-default_background"><strong>오버 샘플링의 장단점</strong></mark></h3><p id="1b44d778-1cdc-81af-9484-dd6396b0bb59" class=""><mark class="highlight-default_background"><strong>장점 : </strong></mark><mark class="highlight-default_background">데이터를 증가시키기 때문에 정보 손실이 없습니다.</mark></p><p id="1b44d778-1cdc-8161-aa28-f0e7d4e028a7" class=""><mark class="highlight-default_background">대부분의 경우 언더 샘플링에 비해 높은 분류 정확도를 보입니다.</mark></p><p id="1b44d778-1cdc-81e7-8d16-edfb8bde7436" class=""><mark class="highlight-default_background"><strong>단점 : </strong></mark><mark class="highlight-default_background">데이터 증가로 인해 계산 시간이 증가할 수 있으며 과적합 가능성이 존재합니다.</mark></p><p id="1b44d778-1cdc-817c-976d-ea949529841f" class=""><mark class="highlight-default_background">노이즈 또는 이상치에 민감합니다.</mark></p><ul id="1b44d778-1cdc-817c-af6f-fc42a7fdf87f" class="toggle"><li><details open=""><summary><mark class="highlight-default_background"><strong>Resampling</strong></mark></summary><p id="1b44d778-1cdc-8142-874b-d9cad2b75d0f" class=""><strong>Resampling 방법은 소수 범주의 데이터 수를 다수 범주의 데이터 수와 비슷해지도록 증가시키는 방법</strong>입니다.</p><figure id="1b44d778-1cdc-81ff-b001-d4f7f2d8298c" class="image"><a href="https://blog.kakaocdn.net/dn/bLFPVl/btraBUa2mJ3/cAMqr8afEfnV6cHePuixAk/img.jpg"><img style="width:355px" src="https://blog.kakaocdn.net/dn/bLFPVl/btraBUa2mJ3/cAMqr8afEfnV6cHePuixAk/img.jpg"/></a></figure><p id="1b44d778-1cdc-81fb-9e72-f16d52349a4e" class="">이때 소수 범주의 데이터는 무작위로 복제됩니다.</p><p id="1b44d778-1cdc-81c0-87a1-fbb283ea19ce" class="">이 방법은 소수 범주에 과적합이 발생할 수 있다는 단점이 있고 이를 보완하기 위해 나온 방법이 SMOTE 방법입니다.</p><ul id="1b44d778-1cdc-81ee-b919-e9f65d7dc82c" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-8165-85fc-fb097d8b9b22" class="code"><code class="language-Python">from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import RandomOverSampler

# 샘플 데이터 생성 (불균형 데이터셋)
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42)
print(f&#x27;Original dataset shape: {Counter(y)}&#x27;)

# 랜덤 오버샘플링 적용
ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_resample(X, y)
print(f&#x27;Resampled dataset shape: {Counter(y_res)}&#x27;)
</code></pre></details></li></ul></details></li></ul><ul id="1b44d778-1cdc-8135-91d8-cc268785fcfe" class="toggle"><li><details open=""><summary><mark class="highlight-default_background"><strong>SMOTE</strong></mark></summary><p id="1b44d778-1cdc-8148-9148-ed5f9a7c3544" class=""><strong>SMOTE 방법은 소수 범주에서 가상의 데이터를 생성하는 방법</strong>입니다.<div class="indented"><p id="1b44d778-1cdc-815a-bf43-d517c3311129" class="">K값을 정한 후 소수 범주에서 임의의 데이터를 선택한 후</p><p id="1b44d778-1cdc-81da-bebe-dc8f69f697bf" class="">선택한 데이터와 가장 가까운 K개의 데이터 중 하나를 무작위로 선정해 Synthetic 공식을 통해 가상의 데이터를 생성하는 방법입니다.</p><p id="1b44d778-1cdc-8177-a70e-f0bc6a0b23f2" class="">이 과정을 소수 범주에 속하는 모든 데이터에 대해 수행하여 가상의 데이터를 생성합니다.</p><figure id="1b44d778-1cdc-81e9-a13b-edeab76fa970" class="image"><a href="https://blog.kakaocdn.net/dn/SsZa2/btrawAYCbu2/EIDSWOVKfMzDrxdDn6A5n1/img.jpg"><img style="width:355px" src="https://blog.kakaocdn.net/dn/SsZa2/btrawAYCbu2/EIDSWOVKfMzDrxdDn6A5n1/img.jpg"/></a></figure><p id="1b44d778-1cdc-813a-a85c-e71712adbdd2" class="">초록색이 생성된 가상 데이터</p><p id="1b44d778-1cdc-811a-9cd4-c39c14ff8832" class="">주의할 점은<strong> K값은 무조건 2 이상이어야</strong> 합니다.</p><p id="1b44d778-1cdc-81c6-b7b3-e89436584cc6" class="">왜냐하면 K가 1일 경우 데이터가 이상한 형태로 늘어나게 됩니다.</p><ul id="1b44d778-1cdc-81a2-911d-f04cd4387a1c" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-81a4-a3bd-dee644ba4e9f" class="code"><code class="language-Python">from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE

# 샘플 데이터 생성 (불균형 데이터셋)
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42)
print(f&#x27;Original dataset shape: {Counter(y)}&#x27;)

# SMOTE 오버샘플링 적용
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)
print(f&#x27;Resampled dataset shape: {Counter(y_res)}&#x27;)
</code></pre></details></li></ul></div></p></details></li></ul><figure id="1b44d778-1cdc-81c1-8d83-d7dfd65fe896" class="image"><a href="image%207.png"><img style="width:399px" src="image%207.png"/></a></figure></li></ol></details></li></ul><ul id="1b44d778-1cdc-8126-99f6-f06183b9ac5f" class="toggle"><li><details open=""><summary><mark class="highlight-default_background">클래스 가중치 조절</mark></summary><p id="1b44d778-1cdc-81bd-81d6-fc2a5f6acc70" class="">oversampling이나 undersampling 모두 데이터를 생성하거나 버리는 방법이기 때문에 overfitting이 발생한다거나 데이터가 낭비되는 문제가 발생할 수 있다. 그래서 데이터는 그대로 냅두고 클래스별 개수를 계산하여 개수가 적은 것에 가중치를 더 많이 부여하게 하여 학습을 진행한다.</p><ul id="1b44d778-1cdc-8144-ab37-c06133aa44dc" class="toggle"><li><details open=""><summary>코드</summary><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1b44d778-1cdc-8195-ab35-dcd3399b5b85" class="code"><code class="language-Python">from sklearn.utils import class_weight

# 클래스 가중치 계산
class_weights = class_weight.compute_sample_weight(class_weight=&#x27;balanced&#x27;, y=y_train)

# 모델에 가중치를 하이퍼 파라미터로 전달
SVM = svc(class_weight = class_weights)</code></pre></details></li></ul></details></li></ul><p id="1b44d778-1cdc-80a8-ba92-e3b24f4905ff" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>